{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# offline habitat map generation using the openEO generated CatBoost models and feature data cubes\n",
    "In this notebook we apply the generated CatBoost models on the openEO generated feature data cubes to run the hierarchical EUNIS habitat mapping. Output are habitat occurrence probability layers for each EUNIS habitat existing in the test sites."
   ],
   "id": "77d099dcf2209e08"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-05T15:41:24.209799Z",
     "start_time": "2024-12-05T15:41:18.371799Z"
    }
   },
   "source": [
    "import platform\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import rioxarray\n",
    "from catboost import CatBoostClassifier\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T15:41:27.041116Z",
     "start_time": "2024-12-05T15:41:27.035960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get input and output root folders\n",
    "if platform.system() == 'Windows':\n",
    "    path_root_models = os.path.normpath(r'\\\\netapp03.vgt.vito.be\\habitat\\slovakia\\openEO_tests\\alpha-1\\2_model_training')\n",
    "    path_root_cubes = os.path.normpath(r'\\\\netapp03.vgt.vito.be\\habitat\\slovakia\\openEO_tests\\alpha-1\\3_datacubes_test-sides')\n",
    "    path_root_out = os.path.normpath(r'\\\\netapp03.vgt.vito.be\\habitat\\slovakia\\openEO_tests\\alpha-1\\4_inference-offline')\n",
    "    path_encoder = os.path.normpath(r'\\\\netapp03.vgt.vito.be\\habitat\\LUT\\EUNIS2007_LUT.csv')\n",
    "else:\n",
    "    path_root_models = os.path.normpath(r'/data/habitat/slovakia/openEO_tests/alpha-1/2_model_training')\n",
    "    path_root_cubes = os.path.normpath(r'/data/habitat/slovakia/openEO_tests/alpha-1/3_datacubes_test-sides')\n",
    "    path_root_out = os.path.normpath(r'/data/habitat/slovakia/openEO_tests/alpha-1/4_inference-offline')\n",
    "    path_encoder = os.path.normpath(r'/data/habitat/LUT/EUNIS2007_LUT.csv')"
   ],
   "id": "3214b642e9be6f4b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### get overview of models and input cubes",
   "id": "f44661fd8d814a33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T15:41:37.253055Z",
     "start_time": "2024-12-05T15:41:36.990140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scan path_root_cubes for .tif files and add Paths to a pandas DataFrame\n",
    "tif_files = glob.glob(os.path.join(path_root_cubes, '**', '*.tif'), recursive=True)\n",
    "df_tif_paths = pd.DataFrame(tif_files, columns=['file_path'])\n",
    "df_tif_paths['basename'] = df_tif_paths['file_path'].apply(lambda x: os.path.basename(x))\n",
    "df_tif_paths = df_tif_paths.join(df_tif_paths['basename'].str.split('_', expand=True).rename(columns={0: 'version', 1: 'site', 2: 'type', 3: 'year', 4: 'tile'}))\n",
    "df_tif_paths['year'] = pd.to_datetime(df_tif_paths['year'], format='year%Y')"
   ],
   "id": "7dfdff63cd463fa7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T15:41:46.423280Z",
     "start_time": "2024-12-05T15:41:46.412321Z"
    }
   },
   "cell_type": "code",
   "source": "df_tif_paths.head()",
   "id": "2455b64b3fd6026d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                           file_path  \\\n",
       "0  \\\\netapp03.vgt.vito.be\\habitat\\slovakia\\openEO...   \n",
       "1  \\\\netapp03.vgt.vito.be\\habitat\\slovakia\\openEO...   \n",
       "2  \\\\netapp03.vgt.vito.be\\habitat\\slovakia\\openEO...   \n",
       "3  \\\\netapp03.vgt.vito.be\\habitat\\slovakia\\openEO...   \n",
       "4  \\\\netapp03.vgt.vito.be\\habitat\\slovakia\\openEO...   \n",
       "\n",
       "                                            basename  version        site  \\\n",
       "0  alpha-1_test-sites_feature-cube_year2021_E468N...  alpha-1  test-sites   \n",
       "1  alpha-1_test-sites_feature-cube_year2021_E468N...  alpha-1  test-sites   \n",
       "2  alpha-1_test-sites_feature-cube_year2021_E470N...  alpha-1  test-sites   \n",
       "3  alpha-1_test-sites_feature-cube_year2021_E470N...  alpha-1  test-sites   \n",
       "4  alpha-1_test-sites_feature-cube_year2021_E472N...  alpha-1  test-sites   \n",
       "\n",
       "           type       year          tile  \n",
       "0  feature-cube 2021-01-01  E468N306.tif  \n",
       "1  feature-cube 2021-01-01  E468N308.tif  \n",
       "2  feature-cube 2021-01-01  E470N306.tif  \n",
       "3  feature-cube 2021-01-01  E470N308.tif  \n",
       "4  feature-cube 2021-01-01  E472N306.tif  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>basename</th>\n",
       "      <th>version</th>\n",
       "      <th>site</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>tile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\\\netapp03.vgt.vito.be\\habitat\\slovakia\\openEO...</td>\n",
       "      <td>alpha-1_test-sites_feature-cube_year2021_E468N...</td>\n",
       "      <td>alpha-1</td>\n",
       "      <td>test-sites</td>\n",
       "      <td>feature-cube</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>E468N306.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\\\netapp03.vgt.vito.be\\habitat\\slovakia\\openEO...</td>\n",
       "      <td>alpha-1_test-sites_feature-cube_year2021_E468N...</td>\n",
       "      <td>alpha-1</td>\n",
       "      <td>test-sites</td>\n",
       "      <td>feature-cube</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>E468N308.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\\\netapp03.vgt.vito.be\\habitat\\slovakia\\openEO...</td>\n",
       "      <td>alpha-1_test-sites_feature-cube_year2021_E470N...</td>\n",
       "      <td>alpha-1</td>\n",
       "      <td>test-sites</td>\n",
       "      <td>feature-cube</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>E470N306.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\\\netapp03.vgt.vito.be\\habitat\\slovakia\\openEO...</td>\n",
       "      <td>alpha-1_test-sites_feature-cube_year2021_E470N...</td>\n",
       "      <td>alpha-1</td>\n",
       "      <td>test-sites</td>\n",
       "      <td>feature-cube</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>E470N308.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\\\netapp03.vgt.vito.be\\habitat\\slovakia\\openEO...</td>\n",
       "      <td>alpha-1_test-sites_feature-cube_year2021_E472N...</td>\n",
       "      <td>alpha-1</td>\n",
       "      <td>test-sites</td>\n",
       "      <td>feature-cube</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>E472N306.tif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T15:41:50.908469Z",
     "start_time": "2024-12-05T15:41:50.842463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scan path_root_models for a .json file and load it into a dictionary\n",
    "json_files = glob.glob(os.path.join(path_root_models, '*.json'))\n",
    "if json_files:\n",
    "    with open(json_files[0], 'r') as f:\n",
    "        model_dict = json.load(f)"
   ],
   "id": "d57b4d0536c3b12c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T15:41:51.888168Z",
     "start_time": "2024-12-05T15:41:51.875503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# transfer the model information into a hierarchical Dataframe\n",
    "# Function to create a DataFrame with MultiIndex from model_dict\n",
    "def create_hierarchical_dataframe(model_dict):\n",
    "    classification_levels = []\n",
    "    habitat_classes = []\n",
    "    model_paths = []\n",
    "\n",
    "    for key, value in model_dict.items():\n",
    "        parts = key.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            classification_level, habitat_class = parts[0], parts[1]\n",
    "            classification_levels.append(classification_level)\n",
    "            habitat_classes.append(habitat_class)\n",
    "            model_paths.append(value)\n",
    "\n",
    "    index = pd.MultiIndex.from_arrays([classification_levels, habitat_classes], names=['classification_level', 'habitat_class'])\n",
    "    df = pd.DataFrame({'model_path': model_paths}, index=index)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Convert keys into a hierarchical DataFrame structure\n",
    "hierarchical_model_df = create_hierarchical_dataframe(model_dict)"
   ],
   "id": "80b1c2feadfed0c7",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T15:41:57.826279Z",
     "start_time": "2024-12-05T15:41:57.813207Z"
    }
   },
   "cell_type": "code",
   "source": "hierarchical_model_df.head()",
   "id": "3c308fa74e10f795",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                           model_path\n",
       "classification_level habitat_class                                                   \n",
       "Level1               class-0        /data/habitat/slovakia/openEO_tests/alpha-1/2_...\n",
       "Level2               class-C        /data/habitat/slovakia/openEO_tests/alpha-1/2_...\n",
       "                     class-D        /data/habitat/slovakia/openEO_tests/alpha-1/2_...\n",
       "                     class-G        /data/habitat/slovakia/openEO_tests/alpha-1/2_...\n",
       "                     class-F        /data/habitat/slovakia/openEO_tests/alpha-1/2_..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>model_path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classification_level</th>\n",
       "      <th>habitat_class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Level1</th>\n",
       "      <th>class-0</th>\n",
       "      <td>/data/habitat/slovakia/openEO_tests/alpha-1/2_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Level2</th>\n",
       "      <th>class-C</th>\n",
       "      <td>/data/habitat/slovakia/openEO_tests/alpha-1/2_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class-D</th>\n",
       "      <td>/data/habitat/slovakia/openEO_tests/alpha-1/2_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class-G</th>\n",
       "      <td>/data/habitat/slovakia/openEO_tests/alpha-1/2_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class-F</th>\n",
       "      <td>/data/habitat/slovakia/openEO_tests/alpha-1/2_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### set some functions for the processing",
   "id": "d0219611806b767e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T15:42:04.205165Z",
     "start_time": "2024-12-05T15:42:04.125948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# habitat dict (encoder) -> raster value to habitat name\n",
    "dhabitat = pd.read_csv(path_encoder, index_col=0).set_index('raster_value')['eunis_code'].to_dict()"
   ],
   "id": "e3c57938bbde86df",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T15:51:46.883767Z",
     "start_time": "2024-12-05T15:51:46.874441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to run the probability prediction\n",
    "# NOTE: this processing is NOT USING block processing since we can still handle a 2000x2000 pixel cube with 253 bands in one go\n",
    "def inference(data: xr.Dataset, row, label_encoder: dict) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Performs inference using a pre-trained CatBoost model on the provided dataset to predict\n",
    "    habitat type probabilities, and then constructs an xarray.Dataset containing these\n",
    "    probabilities. The prediction involves reshaping data to meet model requirements and\n",
    "    converting predictions to a raster format with specified nodata values.\n",
    "\n",
    "    :param data: An xarray.Dataset containing the input data for inference.\n",
    "    :param row: A record containing model path and classification level details.\n",
    "    :param label_encoder: A dictionary mapping class indices to habitat names.\n",
    "    :return: An xarray.Dataset containing the predicted habitat type probabilities\n",
    "        as data variables, with each variable having specified attributes and nodata values.\n",
    "    \"\"\"\n",
    "    # get needed paths\n",
    "    path_catboost_model = os.path.join(row.model_path, 'catboost_v1.cbm')\n",
    "    path_predictors_model = os.path.join(row.model_path, 'predictors.json')\n",
    "\n",
    "    # Replace 'data' with '\\\\netapp03.vgt.vito.be' if platform is Windows\n",
    "    if platform.system() == 'Windows':\n",
    "        path_catboost_model = path_catboost_model.replace('data', r'/netapp03.vgt.vito.be')\n",
    "        path_predictors_model = path_predictors_model.replace('data', r'/netapp03.vgt.vito.be')\n",
    "\n",
    "        # Ensure any '/' in the paths are converted to '\\\\' for Windows compatibility\n",
    "        path_catboost_model = path_catboost_model.replace('/', '\\\\')\n",
    "        path_predictors_model = path_predictors_model.replace('/', '\\\\')\n",
    "\n",
    "    # Load the CatBoost model from the specified path\n",
    "    catboost_model = CatBoostClassifier()\n",
    "    catboost_model.load_model(path_catboost_model)\n",
    "\n",
    "    # extract the feature names needed from json file\n",
    "    with open(path_predictors_model, 'r') as f:\n",
    "        predictors_data = json.load(f)\n",
    "\n",
    "    # set the predictor names as feature names in the catboost model\n",
    "    catboost_model.set_feature_names(predictors_data)\n",
    "\n",
    "    # Filter the data by the variable names specified in predictors_data\n",
    "    filtered_data = data[catboost_model.feature_names_]\n",
    "\n",
    "    # info of original array shape\n",
    "    len_y = filtered_data.sizes['y']\n",
    "    len_x = filtered_data.sizes['x']\n",
    "    len_features = len(filtered_data.data_vars)\n",
    "    # info for result bands (probability layer for the single habitat types of the model)\n",
    "    len_habitats = catboost_model.classes_.shape[0]\n",
    "\n",
    "    ##### here comes the real work\n",
    "    # Predict the probabilities with the CatBoost model on the filtered data\n",
    "    # the Xaaray data is transfered to numpy array with shape [bands, y, x] and has to be reshaped to [y*x, bands] (known as [object_count, feature_count]])\n",
    "    habitat_proba = catboost_model.predict_proba(filtered_data.to_array().values.reshape(len_features,len_y * len_x).T)\n",
    "\n",
    "    # multiply with 100 to prepare for Byte format\n",
    "    habitat_proba = habitat_proba * 100\n",
    "    # now we have to reshape the habitat proba from [y*x, habitat_classes] to rasterio standard [bands, y, x]\n",
    "    habitat_proba = habitat_proba.reshape(len_y, len_x, len_habitats).transpose(2, 0, 1).astype(np.uint8)\n",
    "\n",
    "    ##### create the output xarray.Dataset\n",
    "    # create the correct band names for the probability layers in the order of the catboost_model.classes_\n",
    "    # EUNIS-level_model-name_habitat-name_habitat-raster-value\n",
    "    # e.g. Level1_class-0_habitat-C-30000\n",
    "    # e.g. Level2_class-C_habitat-C1-30100\n",
    "    # e.g. Level3_class-C1_habitat-C1.1-30101\n",
    "    blevel = row.classification_level\n",
    "    bmodel = row.habitat_class\n",
    "    bnames = [f'{blevel}_{bmodel}_habitat-{label_encoder[x]}-{x}' for x in catboost_model.classes_]\n",
    "\n",
    "    # specify the nodata value\n",
    "    nodata_val = 255\n",
    "\n",
    "    # create the xarray.dataset\n",
    "    habitat_proba_ds = xr.Dataset(\n",
    "        {name: (['y', 'x'], habitat_proba[i]) for i, name in enumerate(bnames)},\n",
    "        coords={\n",
    "            'x': data.coords['x'],\n",
    "            'y': data.coords['y']\n",
    "        },\n",
    "        attrs=data.attrs  # Copy over attributes from the original data\n",
    "    )\n",
    "    # Set nodata value for each variable\n",
    "    for var in habitat_proba_ds.data_vars:\n",
    "        habitat_proba_ds[var].rio.write_nodata(nodata_val, inplace=True)\n",
    "    # make sure all possible nan are replaced by new nodata value\n",
    "    habitat_proba_ds = habitat_proba_ds.fillna(nodata_val)\n",
    "    # del the openEO attribute\n",
    "    del habitat_proba_ds.attrs['PROCESSING_SOFTWARE']\n",
    "\n",
    "    # add some DataArray attributes\n",
    "    for var in habitat_proba_ds.data_vars:\n",
    "        habitat_proba_ds[var].encoding['habitat_class'] = var.split('_')[2].split('-')[1]\n",
    "        habitat_proba_ds[var].encoding['class_raster_value'] = var.split('_')[2].split('-')[2]\n",
    "\n",
    "    return habitat_proba_ds"
   ],
   "id": "803f27080e450e91",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## loop over all data cubes of the test sides and the models",
   "id": "1ca6ace838404361"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T15:57:20.098435Z",
     "start_time": "2024-12-05T15:52:35.138068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#loop over all data cubes\n",
    "for cube in tqdm(df_tif_paths.itertuples(), total=len(df_tif_paths), desc='processing data cubes'):\n",
    "    # Load the test_path into a xarray DataArray with bands as variables\n",
    "    data = rioxarray.open_rasterio(cube.file_path, band_as_variable=True, masked=True)\n",
    "    # adapt the band name to the feature names extracted from the metadata\n",
    "    data = data.rename({band:data[band].attrs[\"long_name\"] for band in data})\n",
    "\n",
    "    # init the list holding the Xarray Datasets for the final merge in the end\n",
    "    lDatasets = []\n",
    "\n",
    "    # loop over all models we have to run\n",
    "    for model in tqdm(hierarchical_model_df.reset_index().itertuples(), total=len(hierarchical_model_df), desc='running models', leave=False):\n",
    "        # run the prediction and xarray Dataset generation for the model and add to results\n",
    "        lDatasets.append(inference(data, model, dhabitat))\n",
    "\n",
    "    # merge all model Datasets into one big\n",
    "    result = xr.merge(lDatasets)\n",
    "\n",
    "    # output file name\n",
    "    out_file_name = os.path.join(path_root_out, cube.basename.replace('feature-cube', 'habitat_probabilities'))\n",
    "\n",
    "    # Save habitat_proba_ds as a Cloud Optimized GeoTIFF (COG)\n",
    "    result.rio.to_raster(out_file_name, driver='COG')\n",
    "\n",
    "    # free memory\n",
    "    del data\n",
    "    del lDatasets\n",
    "    del result"
   ],
   "id": "be79ffb86f08ab9c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing data cubes:   0%|          | 0/32 [00:00<?, ?it/s]\n",
      "running models:   0%|          | 0/23 [00:00<?, ?it/s]\u001B[A\n",
      "running models:   4%|▍         | 1/23 [01:31<33:30, 91.40s/it]\u001B[A\n",
      "running models:   9%|▊         | 2/23 [01:47<16:24, 46.87s/it]\u001B[A\n",
      "running models:  13%|█▎        | 3/23 [02:01<10:42, 32.13s/it]\u001B[A\n",
      "running models:  17%|█▋        | 4/23 [02:37<10:40, 33.72s/it]\u001B[A\n",
      "running models:  22%|██▏       | 5/23 [02:46<07:21, 24.52s/it]\u001B[A\n",
      "running models:  26%|██▌       | 6/23 [02:54<05:25, 19.12s/it]\u001B[A\n",
      "running models:  30%|███       | 7/23 [02:56<03:37, 13.58s/it]\u001B[A\n",
      "running models:  35%|███▍      | 8/23 [02:58<02:25,  9.69s/it]\u001B[A\n",
      "running models:  39%|███▉      | 9/23 [03:03<01:54,  8.21s/it]\u001B[A\n",
      "running models:  43%|████▎     | 10/23 [03:06<01:26,  6.62s/it]\u001B[A\n",
      "running models:  48%|████▊     | 11/23 [03:10<01:12,  6.02s/it]\u001B[A\n",
      "running models:  52%|█████▏    | 12/23 [03:17<01:09,  6.33s/it]\u001B[A\n",
      "running models:  57%|█████▋    | 13/23 [03:19<00:48,  4.83s/it]\u001B[A\n",
      "running models:  61%|██████    | 14/23 [03:23<00:40,  4.50s/it]\u001B[A\n",
      "running models:  65%|██████▌   | 15/23 [03:26<00:33,  4.22s/it]\u001B[A\n",
      "running models:  70%|██████▉   | 16/23 [03:45<01:00,  8.59s/it]\u001B[A\n",
      "running models:  74%|███████▍  | 17/23 [03:48<00:40,  6.82s/it]\u001B[A\n",
      "running models:  78%|███████▊  | 18/23 [03:50<00:28,  5.61s/it]\u001B[A\n",
      "running models:  83%|████████▎ | 19/23 [03:52<00:17,  4.42s/it]\u001B[A\n",
      "running models:  87%|████████▋ | 20/23 [03:54<00:11,  3.67s/it]\u001B[A\n",
      "running models:  91%|█████████▏| 21/23 [03:57<00:06,  3.38s/it]\u001B[A\n",
      "running models:  96%|█████████▌| 22/23 [04:01<00:03,  3.53s/it]\u001B[A\n",
      "running models: 100%|██████████| 23/23 [04:02<00:00,  2.94s/it]\u001B[A\n",
      "processing data cubes:   0%|          | 0/32 [04:44<?, ?it/s]  \u001B[A\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c06970e20c4ead1d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weed",
   "language": "python",
   "name": "weed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
