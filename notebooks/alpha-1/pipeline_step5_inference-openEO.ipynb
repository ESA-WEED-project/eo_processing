{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# EUNIS habitat mapping in openEO - run of the inference part to generate habitat occurrence probabilities\n",
    "In this notebook we generate the occurrence probability maps for each EUNIS habitat in the hierarchical CatBoost ML approach."
   ],
   "id": "d0e41e8f3ca13bf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import openeo\n",
    "from openeo.extra.job_management import CsvJobDatabase\n",
    "from eo_processing.utils import laea20km_id_to_extent, reproj_bbox_to_ll\n",
    "from eo_processing.utils.helper import init_connection, location_visu\n",
    "from eo_processing.utils.jobmanager import WeedJobManager\n",
    "from eo_processing.utils.storage import get_gdrive_gdf, print_gdrive_overview, WEED_GDRIVE_Access\n",
    "from eo_processing.openeo.processing import generate_master_feature_cube\n",
    "from eo_processing.config import get_job_options, get_collection_options, get_standard_processing_options\n",
    "import eo_processing.resources\n",
    "import importlib.resources as importlib_resources\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Optional\n",
    "import platform"
   ],
   "id": "b4d580a4186c7305",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### get the alpha-1 test sites from the GDrive (SK and CZ)",
   "id": "e0a1d97e9a8f019d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# init the GDRIVE access\n",
    "# provide the terrascope credentials to access the VITO vault\n",
    "username = 'buchhornm'\n",
    "gdrive = WEED_GDRIVE_Access(username=username, entry_point=\"1k27bitdRp41AtHq1xupyqwKaTLzrMUMu\")"
   ],
   "id": "2f2176b845ccfc05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get an overview of the folder structure\n",
    "print_gdrive_overview(gdrive)"
   ],
   "id": "d3d152be0545b29f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load the test sides into a GeoPandas GeoDataFrame\n",
    "path_SK = gdrive.root + \"/\" + 'SK_test-sites_alpha1_EPSG4326.gpkg'\n",
    "path_CZ = gdrive.root + \"/\" + 'CZ_test-sites_alpha1_EPSG4326.gpkg'\n",
    "# use helper function to load the GDrive file directly\n",
    "gdf_sites = [get_gdrive_gdf(gdrive, path_file) for path_file in [path_SK, path_CZ]]\n",
    "gdf_sites = gpd.GeoDataFrame(pd.concat(gdf_sites, ignore_index=True))"
   ],
   "id": "d3e753e9b880f052",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### specify space context for the openEO run\n",
    "The space context is set by intersecting the test sites (AOIs) with the openEO EU 20x20km tiling grid. Since the tiling grid has a smaller file size, it was also stored in the resources of the eo_processing package as well as on the GDrive folder. <br>"
   ],
   "id": "efa0dc307998dc8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get the LAEA-20km grid saved as EPSG:4326 vector\n",
    "grid = importlib_resources.files(eo_processing.resources).joinpath('LAEA-20km.gpkg')\n",
    "gdf_grid = gpd.read_file(os.path.normpath(grid))"
   ],
   "id": "6e1d57410d87e017",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# spatial intersect and retrieve the tile_ID's of the LAEA-20km grid (convert AOI to EPSG:4326 if needed)\n",
    "gdf_aoi = gdf_grid[gdf_grid.intersects(gdf_sites.union_all())]"
   ],
   "id": "d0abb2193cde3137",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# visualization of the selected- tiles in the tiling grid which needs to be processed to cover the AOI\n",
    "location_visu(gdf_aoi, zoom=True, region='EU', label=True)"
   ],
   "id": "a8a83f516d42406",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Integration of the MultiBackendJobManager\n",
   "id": "77c59282dcac93d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### creating the jobs dataframe\n",
    "The `MultiBackendJobManager` uses a jobs database to set-up, start and monitor all desired jobs. We are using Pandas/GeoPandas to create a DataFrame storing the job and all parameters which should adapted for each job.\n",
    "\n",
    "In this example we can directly use the GeoDataFrame of the tiling grid since after the intersection with the AOI is contains only the remaining block extents to process."
   ],
   "id": "669e50be98ffb9f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_job_dataframe(gdf: gpd.GeoDataFrame, year: int, file_name_base: str, target_crs: Optional[int] = None) -> gpd.GeoDataFrame:\n",
    "\n",
    "    columns = ['name', 'target_epsg', 'file_prefix', 'start_date', 'end_date', 'geometry']\n",
    "    dtypes = {'name': 'string', 'target_epsg': 'UInt16', 'file_prefix': 'string', 'start_date': 'string', 'end_date': 'string', 'geometry': 'geometry'}\n",
    "\n",
    "    job_df = gdf.copy()\n",
    "\n",
    "    # the time context is given by start and end date\n",
    "    job_df['start_date'] = f'{year}-01-01'\n",
    "    job_df['end_date'] = f'{year+1}-01-01'  # the end is always exclusive\n",
    "\n",
    "    # adding the output file name pre-fix\n",
    "    job_df['file_prefix'] = job_df.apply(lambda row: f'{file_name_base}_EUNIS-habitat-proba-cube_year{year}_{row['name']}', axis=1)\n",
    "\n",
    "    # set the target epsg\n",
    "    if target_crs is None:\n",
    "        job_df['target_epsg'] = 3035\n",
    "    else:\n",
    "        raise NotImplementedError('target_crs is not yet implemented')\n",
    "\n",
    "    return job_df.astype(dtypes)"
   ],
   "id": "cf83d458b88a909f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# we set here also the time context (processing year)\n",
    "job_df = create_job_dataframe(gdf_aoi, 2021, 'alpha-1_test-sites')"
   ],
   "id": "6a07da8fd722d9d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# show the jobs_database before conversion into a JobDatabaseInterface\n",
    "job_df.head()"
   ],
   "id": "853e97aba7fbb158",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### creating the job `start_job` function\n",
    "The next step is to define a `start_job` function. This function will instruct the `MultiBackendJobManager` on how to initiate a new job on the selected backend. The `start_job` functionality should adhere to the following structure _start_job(row: pd.Series, connection: openeo.Connection, **kwargs)_."
   ],
   "id": "a7241884296652fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def inference(row: gpd.GeoSeries, connection: openeo.Connection, provider: str , **kwgs) -> openeo.BatchJob:\n",
    "    \"\"\"Start a new job using the specified row out of the jobs GeoDataFrame and connection.\"\"\"\n",
    "\n",
    "    # Get the variable parameters from the dataframe\n",
    "    tileID = row[\"name\"]\n",
    "    start = row[\"start_date\"]\n",
    "    end = row[\"end_date\"]\n",
    "    file_name = row[\"file_prefix\"]\n",
    "    epsg = int(row[\"target_epsg\"])\n",
    "\n",
    "    # convert the row name into a openEO bbox dict giving the spatial extent of the job\n",
    "    processing_extent = laea20km_id_to_extent(tileID)\n",
    "\n",
    "    # define job_options, processing_options,  and collection_options\n",
    "    job_options = get_job_options(provider=provider)\n",
    "    collection_options = get_collection_options(provider=provider)\n",
    "    processing_options = get_standard_processing_options(provider=provider, task='feature_generation')\n",
    "\n",
    "    # adapt the epsg to the processing grid\n",
    "    processing_options.update(target_crs = epsg)\n",
    "\n",
    "    #create the progress graph for the feature cube\n",
    "    # define the S1/S2 processed feature cube (Note: do not set spatial extent since we had it over in the end)\n",
    "    data_cube = generate_master_feature_cube(connection,\n",
    "                                             None,\n",
    "                                             start,\n",
    "                                             end,\n",
    "                                             **collection_options,\n",
    "                                             **processing_options)\n",
    "\n",
    "    # now we merge in the NON ON-DEMAND processed features (DEM and WENR features)\n",
    "    # load the DEM from a CDSE collection\n",
    "    DEM = connection.load_collection(\n",
    "        \"COPERNICUS_30\",\n",
    "        bands=[\"DEM\"])\n",
    "    # reduce the temporal domain since copernicus_30 collection is \"special\" and feature only are one time stamp\n",
    "    DEM = DEM.reduce_dimension(dimension='t', reducer=lambda x: x.last(ignore_nodata=True))\n",
    "    # resample the cube to 10m and EPSG of corresponding 20x20km grid tile\n",
    "    DEM = DEM.resample_spatial(projection=processing_options['target_crs'],\n",
    "                               resolution=processing_options['resolution'],\n",
    "                               method=\"bilinear\")\n",
    "    # merge into the S1/S2 data cube\n",
    "    data_cube = data_cube.merge_cubes(DEM)\n",
    "\n",
    "    # load the WERN features from public STAC\n",
    "    WENR = connection.load_stac(\"https://stac.openeo.vito.be/collections/wenr_features\")\n",
    "    # resample the cube to 10m and EPSG of corresponding 20x20km grid tile\n",
    "    WENR = WENR.resample_spatial(projection=processing_options['target_crs'],\n",
    "                                 resolution=processing_options['resolution'],\n",
    "                                 method=\"near\")\n",
    "    # drop the time dimension\n",
    "    try:\n",
    "        WENR = WENR.drop_dimension('t')\n",
    "    except:\n",
    "        # workaround if we still have the client issues with the time dimensions for STAC dataset with only one time stamp\n",
    "        WENR.metadata = WENR.metadata.add_dimension(\"t\", label=None, type=\"temporal\")\n",
    "        WENR = WENR.drop_dimension('t')\n",
    "    # merge into the S1/S2 data cube\n",
    "    data_cube = data_cube.merge_cubes(WENR)\n",
    "\n",
    "    # filter spatial the whole cube\n",
    "    data_cube = data_cube.filter_bbox(processing_extent)\n",
    "\n",
    "    ####### fill here the inference workflow #####\n",
    "    # ToDo: filled by Hans with the multiple catboost onnx inference workflow to create a datacube for the habitat probabilities from each applied model\n",
    "    # format bands: Byte\n",
    "    # nodata: 255\n",
    "    # names bands: L#_habitat-#\n",
    "    ######### end inference ######\n",
    "\n",
    "    # create the job progress graph\n",
    "    job = inference_cube.create_job(title=file_name,\n",
    "                               description=f'generation of full feature data cube for tile {tileID}',\n",
    "                               out_format='GTiff',\n",
    "                               job_options=job_options,\n",
    "                               filename_prefix=file_name,\n",
    "                               )\n",
    "    return job"
   ],
   "id": "723c38e5480bbecd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### set the root output folder",
   "id": "15075bad7c155da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# define the output folder (make sure you adapt this to your folder structure)\n",
    "if platform.system() == 'Windows':\n",
    "    out_root = os.path.normpath(r'\\\\netapp03.vgt.vito.be\\habitat\\slovakia\\openEO_tests\\alpha-1\\5_inference-openEO')\n",
    "else:\n",
    "    out_root = os.path.normpath(r'/data/habitat/slovakia/openEO_tests/alpha-1/5_inference-openEO')\n",
    "os.makedirs(out_root, exist_ok=True)"
   ],
   "id": "a634beb3757e68b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### run the jobs\n",
    "With our jobs database and job definition set up, we can now run the jobs using the `MultiBackendJobManager`. This involves defining a path to where we will store the job tracker which contains the job statuses and metadata.\n",
    "<br> <br>\n",
    "Note: If the specified job tracker path, points to an existing jobs database (either a .csv or .parquet file), that file will be used as the job tracker, and the input jobs database will be disregarded."
   ],
   "id": "4009cb43c746aad7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# generate a unique name for the job tracker database (here: CSV file)\n",
    "job_tracker = os.path.normpath(os.path.join(out_root, 'job_tracker_inference-openEO.csv'))"
   ],
   "id": "18160985d592b55a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# init of MultiBackendJobManager (A viz switch has been added to enable vizualization of status)\n",
    "manager = WeedJobManager(root_dir=out_root, poll_sleep=30, viz=True)"
   ],
   "id": "8758036ed7c1c9fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create a connection to backend and add this active backend to the jobmanager\n",
    "backend = 'cdse'  # also possible: cdse-stagging, terrascope, development, creodias, sentinelhub\n",
    "connection = init_connection(backend)\n",
    "manager.add_backend(backend, connection=connection, parallel_jobs=6)"
   ],
   "id": "c0ca1fb53ac17b21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# due to a bug in the JobManager class we first have to create the CSVJobDatabase by ourself and write it to disk so that we get all custom columns!\n",
    "job_db = CsvJobDatabase(path=job_tracker)\n",
    "if job_db.exists():\n",
    "    print(f\"Resuming `run_jobs` from existing {job_db}\")\n",
    "else:\n",
    "    df = manager._normalize_df(job_df)\n",
    "    job_db.persist(df)\n",
    "\n",
    "manager.run_jobs(start_job=featurecube_generation, job_db=job_db)"
   ],
   "id": "bdf4aee941794b2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### validate that all needed feature data cubes are generated",
   "id": "d5645ed4e7fb0e7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load the job_tracker file into a pandas DataFrame\n",
    "df_tracker = pd.read_csv(job_tracker)"
   ],
   "id": "5dbc09172d2312a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check if all values in column \"status\" have the value \"finished\"\n",
    "all_finished = df_tracker['status'].eq('finished').all()\n",
    "if all_finished:\n",
    "    print(\"All jobs are finished.\")\n",
    "else:\n",
    "    print(\"Some jobs are not finished. Reset the non finished status rows to 'not_started' flag and restart the manager.run_jobs() line manually.\")"
   ],
   "id": "c8e3126b9b6df5f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print the costs of the full job\n",
    "print(f'The feature datacube generation {df_tracker.shape[0]} 20x20km tiles has cost overall {df_tracker.cost.sum()} credits ({df_tracker.cost.sum() * 0.01} EURO). ')"
   ],
   "id": "216514dda740c816",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get the list of links to the GeoTif files\n",
    "df_tracker['path_result'] = df_tracker['file_prefix'].apply(lambda x: os.path.normpath(os.path.join(out_root, f'{x}.tif')))\n",
    "path_Results = df_tracker['path_result'].tolist()"
   ],
   "id": "b7c429f2d4b033c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Verify that all GeoTIFF files in path_Results exist\n",
    "for file_path in path_Results:\n",
    "    assert os.path.exists(file_path), f\"File not found: {file_path}\""
   ],
   "id": "cfa7c8d0ccbcc7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "23a15420e44cd4e5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
