{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fc2e44-2121-4ff2-a2f5-1c36a4f69d8f",
   "metadata": {},
   "source": [
    "# WEED inference\n",
    "In this notebook we will showcase how we couple the EO processing with ONNX ML inference within weed. \n",
    "\n",
    "The way we operate is by first lazy loading a cube which contains every enabled training feature as a band. \n",
    "Next we read from the model stored on an openEO accesible storage site, on which features it was trained. \n",
    "\n",
    "It is important that users add this information to the stored models. There is code provided in onnx_model_utilities to showcase how it can be done. This code was specialized for obtaining the relevant information from a json file and adding it into the onnx metadata. As the project continues this approach will change since model training will also be streamlined within the WEED operation. \n",
    "\n",
    "note: it is important to store your model in a zip file as shown here, as openEO has a preprogrammed way of unzipping dependency folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8243ee19-d745-4958-97a4-6e67075953ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import openeo\n",
    "\n",
    "sys.path.append(os.path.abspath('C:/Git_projects/eo_processing/src'))\n",
    "\n",
    "from eo_processing.utils.helper import init_connection, getUDFpath\n",
    "from eo_processing.utils.onnx_model_utilities import get_training_features_from_model\n",
    "\n",
    "from eo_processing.openeo.processing import generate_master_feature_cube\n",
    "from eo_processing.config import get_job_options, get_collection_options,  get_standard_processing_options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b125488-ec56-4954-b44a-8dda843e7f12",
   "metadata": {},
   "source": [
    "Connect to openEO processing backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d33644bc-07e4-4733-841e-761b6772b0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated using refresh token.\n"
     ]
    }
   ],
   "source": [
    "backend = 'cdse' \n",
    "# establish the connection to the selected backend\n",
    "connection = init_connection(backend)\n",
    "\n",
    "job_options = get_job_options(provider=backend)\n",
    "collection_options = get_collection_options(provider=backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae12555-8963-4510-af03-73e89ecaae60",
   "metadata": {},
   "source": [
    "### specify space & time context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e1c84a6-9aa2-4efc-aa49-fa59782731a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the time context is given by start and end date\n",
    "start = '2021-01-01'\n",
    "end = '2021-02-01'   # the end is always exclusive\n",
    "AOI = {'east': 4832000, 'south': 2818000, 'west': 4831000, 'north': 2819000, 'crs': 'EPSG:3035'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e876841-8a9e-4d2d-a45f-4ca240643534",
   "metadata": {},
   "source": [
    "Below we initiate the job settings, it must be noted that these have not been optimized yet for the UDF inference workflow. \n",
    "\n",
    "Next we also read out the metadata of the provided model to extrac the features a model was trained on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66e5eb29-0bf2-4eab-9128-33786a011de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_options: {'driver-memory': '8G', 'driver-memoryOverhead': '5G', 'driver-cores': '1', 'executor-memory': '1500m', 'executor-memoryOverhead': '2500m', 'executor-cores': '1', 'max-executors': '25', 'soft-errors': 'true', 'executor-request-cores': '800m', 'executor-threads-jvm': '7', 'logging-threshold': 'info', 'udf-dependency-archives': ['https://s3.waw3-1.cloudferro.com/swift/v1/project_dependencies/onnx_dependencies_1.16.3.zip#onnx_deps']}\n",
      "collection_options: {'S2_collection': 'SENTINEL2_L2A', 'S1_collection': 'SENTINEL1_GRD'}\n",
      "processing_options: {'provider': 'cdse', 's1_orbitdirection': 'DESCENDING', 'target_crs': 3035, 'resolution': 10.0, 'time_interpolation': False, 'ts_interval': 'dekad', 'SLC_masking_algo': 'mask_scl_dilation', 'S2_bands': ['B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B11', 'B12'], 'optical_vi_list': ['NDVI', 'AVI', 'CIRE', 'NIRv', 'NDMI', 'NDWI', 'BLFEI', 'MNDWI', 'NDVIMNDWI', 'S2WI', 'S2REP', 'IRECI'], 'radar_vi_list': ['VHVVD', 'VHVVR', 'RVI'], 'S2_scaling': [0, 10000, 0, 1.0], 'S1_db_rescale': True, 'append': True}\n",
      "ML INPUT features: ['BLFEI_p98', 'NDVIMNDWI_mean', 'MNDWI_mean', 'B12_iqr', 'CIRE_p98', 'VHVVR_median', 'NDVI_iqr', 'NDMI_p2', 'VH_mean', 'VHVVR_sd', 'B05_p25', 'S2REP_p98', 'MNDWI_p98', 'VH_iqr', 'B07_p25', 'NDMI_sd', 'IRECI_p98', 'BLFEI_sd', 'NDVIMNDWI_median', 'B08_mean', 'B11_sd', 'S2WI_median', 'B02_p75', 'B06_p75', 'NIRv_sd', 'NIRv_mean', 'AVI_iqr', 'B05_p98', 'VHVVD_sd', 'NDWI_p2', 'B06_mean', 'B11_p98', 'VHVVR_iqr', 'B03_p75', 'RVI_mean', 'NDWI_p98', 'B8A_p98', 'VHVVD_mean', 'VH_p2', 'AVI_p75', 'S2REP_median', 'VHVVD_iqr', 'B03_sum', 'MNDWI_median', 'B06_median', 'VV_p75', 'AVI_p25', 'NIRv_p25', 'VH_p25', 'B12_sum', 'CIRE_p25', 'VHVVR_p75', 'B03_p2', 'B11_median', 'B05_median', 'S2WI_iqr', 'AVI_mean', 'IRECI_sum', 'B08_iqr', 'VHVVR_sum', 'IRECI_iqr', 'BLFEI_sum', 'S2WI_p2', 'B11_sum', 'B06_iqr']\n",
      "ML OUTPUT features: ['predicted_label', 'prob_class_30000', 'prob_class_40000', 'prob_class_50000', 'prob_class_60000', 'prob_class_70000', 'prob_class_80000', 'prob_class_90000', 'prob_class_100000', 'prob_class_110000']\n"
     ]
    }
   ],
   "source": [
    "# we link towards the used model\n",
    "DEPENDENCY_URL = \"https://s3.waw3-1.cloudferro.com/swift/v1/project_dependencies/onnx_dependencies_1.16.3.zip\"\n",
    "MODEL_URL = \"https://s3.waw3-1.cloudferro.com/swift/v1/weed/catboost_models/model_1.onnx\"\n",
    "\n",
    "\n",
    "# we call again the standard processing options for feature generation\n",
    "processing_options = get_standard_processing_options(provider=backend, task='feature_generation')\n",
    "\n",
    "#add the udf dependencies into the job options\n",
    "job_options[\"udf-dependency-archives\"] = [\n",
    "            f\"{DEPENDENCY_URL}#onnx_deps\"]\n",
    "\n",
    "# Get the feature list from the ONNX model inside the zip file\n",
    "metadata = get_training_features_from_model(MODEL_URL)\n",
    "INPUT_BANDS = metadata['input_features']\n",
    "OUTPUT_BANDS = metadata['output_features']\n",
    "\n",
    "# just print for an overview\n",
    "print(f'job_options: {job_options}')\n",
    "print(f'collection_options: {collection_options}')\n",
    "print(f'processing_options: {processing_options}')\n",
    "print(f'ML INPUT features: {INPUT_BANDS}')\n",
    "print(f'ML OUTPUT features: {OUTPUT_BANDS}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1a2ce-3c14-417e-82b9-9f7e0537bb2b",
   "metadata": {},
   "source": [
    "Here we 'master cube' which contains (up till now) 237 bands which could have been used as training features. This cube is 'lazy loaded' in the sense its not fully loaded into memory. \n",
    "\n",
    "With the filter bands operation, we only keep the (65) bands in the cube on which our ML model was trained on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b41f22-0299-4b04-80e1-2bb996ff7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the openEO pipeline to use\n",
    "data_cube = generate_master_feature_cube(connection,\n",
    "                                   AOI,\n",
    "                                   start,\n",
    "                                   end,\n",
    "                                   **collection_options,\n",
    "                                   **processing_options)\n",
    "\n",
    "data_cube = data_cube.filter_bands(INPUT_BANDS)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f76cbf28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Git_projects\\\\eo_processing\\\\src\\\\eo_processing\\\\resources\\\\udf_catboost_inference.py'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getUDFpath('udf_catboost_inference.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7e7d7",
   "metadata": {},
   "source": [
    "Verify that the bands of the input cube match with those of our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3457f21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00 Job 'j-241121a187634356a2c200cbb2e46c29': send 'start'\n",
      "0:00:15 Job 'j-241121a187634356a2c200cbb2e46c29': created (progress 0%)\n",
      "0:00:21 Job 'j-241121a187634356a2c200cbb2e46c29': created (progress 0%)\n",
      "0:00:27 Job 'j-241121a187634356a2c200cbb2e46c29': created (progress 0%)\n",
      "0:00:35 Job 'j-241121a187634356a2c200cbb2e46c29': created (progress 0%)\n",
      "0:00:45 Job 'j-241121a187634356a2c200cbb2e46c29': created (progress 0%)\n",
      "0:00:58 Job 'j-241121a187634356a2c200cbb2e46c29': created (progress 0%)\n",
      "0:01:13 Job 'j-241121a187634356a2c200cbb2e46c29': queued (progress 0%)\n",
      "0:01:33 Job 'j-241121a187634356a2c200cbb2e46c29': queued (progress 0%)\n",
      "0:01:57 Job 'j-241121a187634356a2c200cbb2e46c29': queued (progress 0%)\n",
      "0:02:27 Job 'j-241121a187634356a2c200cbb2e46c29': queued (progress 0%)\n",
      "0:03:05 Job 'j-241121a187634356a2c200cbb2e46c29': queued (progress 0%)\n",
      "0:03:52 Job 'j-241121a187634356a2c200cbb2e46c29': queued (progress 0%)\n",
      "0:04:51 Job 'j-241121a187634356a2c200cbb2e46c29': queued (progress 0%)\n",
      "0:05:52 Job 'j-241121a187634356a2c200cbb2e46c29': queued (progress 0%)\n",
      "0:06:52 Job 'j-241121a187634356a2c200cbb2e46c29': running (progress N/A)\n",
      "0:07:52 Job 'j-241121a187634356a2c200cbb2e46c29': running (progress N/A)\n",
      "0:08:53 Job 'j-241121a187634356a2c200cbb2e46c29': running (progress N/A)\n",
      "0:09:54 Job 'j-241121a187634356a2c200cbb2e46c29': error (progress N/A)\n",
      "Your batch job 'j-241121a187634356a2c200cbb2e46c29' failed. Error logs:\n",
      "[{'id': '[1732197799787, 308206]', 'time': '2024-11-21T14:03:19.787Z', 'level': 'error', 'message': 'Failed extracting and installing UDF dependencies: Unclosed array (at line 3, column 1)'}, {'id': '[1732197924638, 677959]', 'time': '2024-11-21T14:05:24.638Z', 'level': 'error', 'message': 'Task 0 in stage 40.0 failed 4 times; aborting job'}, {'id': '[1732197924642, 21410]', 'time': '2024-11-21T14:05:24.642Z', 'level': 'error', 'message': 'Stage error: Job aborted due to stage failure: Task 0 in stage 40.0 failed 4 times, most recent failure: Lost task 0.3 in stage 40.0 (TID 268) (10.42.169.188 executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\\n    process()\\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\\n    serializer.dump_stream(out_iter, outfile)\\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\\n    for obj in iterator:\\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\\n    return f(*args, **kwargs)\\n  File \"/opt/openeo/lib/python3.8/site-packages/openeogeotrellis/utils.py\", line 57, in memory_logging_wrapper\\n    return function(*args, **kwargs)\\n  File \"/opt/openeo/lib/python3.8/site-packages/epsel.py\", line 44, in wrapper\\n    return _FUNCTION_POINTERS[key](*args, **kwargs)\\n  File \"/opt/openeo/lib/python3.8/site-packages/epsel.py\", line 37, in first_time\\n    return f(*args, **kwargs)\\n  File \"/opt/openeo/lib/python3.8/site-packages/openeogeotrellis/geopysparkdatacube.py\", line 784, in tile_function\\n    result_data = run_udf_code(code=udf_code, data=data)\\n  File \"/opt/openeo/lib/python3.8/site-packages/openeogeotrellis/udf.py\", line 57, in run_udf_code\\n    return openeo.udf.run_udf_code(code=code, data=data)\\n  File \"/opt/openeo/lib/python3.8/site-packages/openeo/udf/run_code.py\", line 149, in run_udf_code\\n    module = load_module_from_string(code)\\n  File \"/opt/openeo/lib/python3.8/site-packages/openeo/udf/run_code.py\", line 61, in load_module_from_string\\n    exec(code, globals)\\n  File \"<string>\", line 14, in <module>\\nModuleNotFoundError: No module named \\'onnx\\'\\n\\n\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\\n\\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\\n\\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\\n\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\\n\\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\\n\\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\\n\\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\\n\\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\\n\\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\\n\\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\\n\\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\\n\\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)\\n\\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)\\n\\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)\\n\\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\\n\\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\\nDriver stacktrace:'}, {'id': '[1732197926351, 698113]', 'time': '2024-11-21T14:05:26.351Z', 'level': 'error', 'message': 'OpenEO batch job failed: UDF exception while evaluating processing graph. Please check your user defined functions.   File \"/opt/openeo/lib/python3.8/site-packages/openeo/udf/run_code.py\", line 149, in run_udf_code\\n    module = load_module_from_string(code)\\n  File \"/opt/openeo/lib/python3.8/site-packages/openeo/udf/run_code.py\", line 61, in load_module_from_string\\n    exec(code, globals)\\n  File \"<string>\", line 14, in <module>\\nModuleNotFoundError: No module named \\'onnx\\''}]\n",
      "Full logs can be inspected in an openEO (web) editor or with `connection.job('j-241121a187634356a2c200cbb2e46c29').logs()`.\n"
     ]
    },
    {
     "ename": "JobFailedException",
     "evalue": "Batch job 'j-241121a187634356a2c200cbb2e46c29' didn't finish successfully. Status: error (after 0:09:55).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJobFailedException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#run inference\u001b[39;00m\n\u001b[0;32m     16\u001b[0m output \u001b[38;5;241m=\u001b[39m catboost_classification\u001b[38;5;241m.\u001b[39mrename_labels(dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbands\u001b[39m\u001b[38;5;124m\"\u001b[39m,target\u001b[38;5;241m=\u001b[39m OUTPUT_BANDS)\n\u001b[1;32m---> 18\u001b[0m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput.nc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\VROMPAYH\\AppData\\Local\\anaconda3\\envs\\weed_env\\lib\\site-packages\\openeo\\rest\\datacube.py:2242\u001b[0m, in \u001b[0;36mDataCube.execute_batch\u001b[1;34m(self, outputfile, out_format, print, max_poll_interval, connection_retry_interval, job_options, validate, **format_options)\u001b[0m\n\u001b[0;32m   2239\u001b[0m     out_format \u001b[38;5;241m=\u001b[39m guess_format(outputfile)\n\u001b[0;32m   2241\u001b[0m job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_job(out_format\u001b[38;5;241m=\u001b[39mout_format, job_options\u001b[38;5;241m=\u001b[39mjob_options, validate\u001b[38;5;241m=\u001b[39mvalidate, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_options)\n\u001b[1;32m-> 2242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_synchronous\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_poll_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_poll_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_retry_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_retry_interval\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\VROMPAYH\\AppData\\Local\\anaconda3\\envs\\weed_env\\lib\\site-packages\\openeo\\rest\\job.py:242\u001b[0m, in \u001b[0;36mBatchJob.run_synchronous\u001b[1;34m(self, outputfile, print, max_poll_interval, connection_retry_interval)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_synchronous\u001b[39m(\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;28mself\u001b[39m, outputfile: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;28mprint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mprint\u001b[39m, max_poll_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, connection_retry_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m\n\u001b[0;32m    240\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchJob:\n\u001b[0;32m    241\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Start the job, wait for it to finish and download result\"\"\"\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_and_wait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_poll_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_poll_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_retry_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_retry_interval\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# TODO #135 support multi file result sets too?\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m outputfile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\VROMPAYH\\AppData\\Local\\anaconda3\\envs\\weed_env\\lib\\site-packages\\openeo\\rest\\job.py:324\u001b[0m, in \u001b[0;36mBatchJob.start_and_wait\u001b[1;34m(self, print, max_poll_interval, connection_retry_interval, soft_error_max)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogs(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mERROR))\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull logs can be inspected in an openEO (web) editor or with `connection.job(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_id\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m).logs()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    323\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JobFailedException(\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch job \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_id\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt finish successfully. Status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    326\u001b[0m         job\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    327\u001b[0m     )\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mJobFailedException\u001b[0m: Batch job 'j-241121a187634356a2c200cbb2e46c29' didn't finish successfully. Status: error (after 0:09:55)."
     ]
    }
   ],
   "source": [
    "#source: https://github.com/clausmichele/openEO_photovoltaic/blob/main/udf_inference/openeo_pv_farms_inference_udf.ipynb\n",
    "\n",
    "#we pass the model url as context information within the UDF\n",
    "udf  = openeo.UDF.from_file(\n",
    "        getUDFpath('udf_catboost_inference.py'),\n",
    "        context={\n",
    "            \"model_url\": MODEL_URL\n",
    "                }\n",
    ")\n",
    "\n",
    "# Apply the UDF to the data cube.\n",
    "catboost_classification = data_cube.apply(\n",
    "    process=udf)\n",
    "\n",
    "#run inference\n",
    "output = catboost_classification.rename_labels(dimension=\"bands\",target= OUTPUT_BANDS)\n",
    "\n",
    "output.execute_batch(\"output.nc\", job_options=job_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weed_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
