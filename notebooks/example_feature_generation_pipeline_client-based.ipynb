{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notebook to show feature extraction for bigger spatial areas via client-side block processing\n",
    "In this example we focus on EU-27 and an area in Slovakia. The prepared tiling grid is generated in the wished output CRS EPSG:3035 (LAEA). The tiling (block processing) is happening on client side.\n",
    "<br><br>\n",
    "We will demonstrate how to use the openEO `MultiBackendJobManager` to set-up and track multiple jobs at once using OpenEO. <br> <br>\n",
    "Key features <br>\n",
    "Job Tracking: Keep track of jobs their statuses and results across different backends. <br>\n",
    "Error Handling: Customizable handling of job errors and completed jobs. <br>\n",
    "Database Support: Persist job metadata using CSV or Parquet files, allowing you to resume tracking after interruptions.<br><br>\n",
    "The error handling in this example is basic - by restarting the script with the same settings the jobmanager will continue with `not_started` jobs."
   ],
   "id": "cde3d3f03faf8fc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import openeo\n",
    "from openeo.extra.job_management import CsvJobDatabase\n",
    "\n",
    "from eo_processing.utils import laea20km_id_to_extent, reproj_bbox_to_ll\n",
    "from eo_processing.utils.helper import init_connection, location_visu, convert_to_list, string_to_dict\n",
    "from eo_processing.utils.jobmanager import WeedJobManager, create_job_dataframe\n",
    "from eo_processing.utils.storage import WEED_storage\n",
    "from eo_processing.utils.geoprocessing import AOI_tiler\n",
    "from eo_processing.openeo.processing import generate_master_feature_cube\n",
    "from eo_processing.config import get_job_options, get_collection_options, get_standard_processing_options, generate_storage_options\n",
    "import eo_processing.resources\n",
    "import importlib.resources as importlib_resources\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely import wkt\n",
    "import time"
   ],
   "id": "8b425dbd1962e367"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### specify space and time context",
   "id": "9756873c70f407b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# the time context is given by start and end date\n",
    "year = 2021\n",
    "start = f'{year}-01-01'\n",
    "end = f'{year+1}-01-01'   # the end is always exclusive\n",
    "\n",
    "# the space context is defined as a bounding box dictionary with south,west,north,east and crs\n",
    "# we take as example the AOI of Slovakia Devin_Lake (wetlands)\n",
    "AOI = {'east': 4800000, 'south': 2820000, 'west': 4831000, 'north': 2830000, 'crs': 'EPSG:3035'}"
   ],
   "id": "62895f85a0b00fde"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### intersect the AOI with the tiling grid to specify the block processing extents",
   "id": "3c8964e36343cc53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# spatial intersect and retrieve the tile_ID's of the LAEA-20km grid (convert AOI to EPSG:4326)\n",
    "gdf_aoi = AOI_tiler(reproj_bbox_to_ll(AOI),'EU')"
   ],
   "id": "b0d3821ab8f92c47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# visualization of the selected- tiles in the tiling grid which needs to be processed to cover the AOI\n",
    "location_visu(gdf_aoi, zoom=True, region='EU', label=True)"
   ],
   "id": "e9a078cae60c10eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### set the root output folder",
   "id": "47d04653ca4842e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define the output folder (make sure you adapt this to your folder structure)\n",
    "test_num = 12\n",
    "out_root = os.path.normpath(r'/home/deroob/Private/WEED/tests')\n",
    "out_root = os.path.join(out_root, f'test_v{str(test_num)}')\n",
    "os.makedirs(out_root, exist_ok=True)\n",
    "#define if S3 workspace is needed\n",
    "workspace_path = f\"tests/Bert/Bert_test_v{str(test_num)}\"\n",
    "storage = WEED_storage(username='deroob',s3_bucket='test')\n",
    "storage_options = generate_storage_options(workspace_export = True, S3_prefix=workspace_path, local_S3_needed = True, storage=storage)\n",
    "storage_options = None\n"
   ],
   "id": "6bf7b92310576980"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### integrating the MultiBackendJobManager\n",
   "id": "79c931995ad77fc4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### creating the jobs database\n",
    "The `MultiBackendJobManager` uses a jobs database to set-up, start and monitor all desired jobs. We are using Pandas/GeoPandas to create a DataFrame storing the job and all parameters which should adapted for each job.\n",
    "\n",
    "In this example we can directly use the GeoDataFrame of the tiling grid since after the intersection with the AOI is contains only the remaining block extents to process."
   ],
   "id": "108b4a292c0046a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "jobs_database = create_job_dataframe(gdf=gdf_aoi.copy(),year=2021,file_name_base = 'balbla', processing_type= 'feature', version = f\"v{test_num}\",\n",
    "                           storage_options =storage_options, organization_id = 4938 )"
   ],
   "id": "99277cbfba0dd393"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "jobs_database = jobs_database.iloc[[4]]\n",
    "jobs_database"
   ],
   "id": "45b705d41ba838da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### creating the job `start_job` function\n",
    "The next step is to define a `start_job` function. This function will instruct the `MultiBackendJobManager` on how to initiate a new job on the selected backend. The `start_job` functionality should adhere to the following structure _start_job(row: pd.Series, connection: openeo.Connection, **kwargs)_."
   ],
   "id": "b733915d07b6a443"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from openeo.rest.datacube import THIS \n",
    "def start_job(row: gpd.GeoSeries, connection: openeo.Connection, provider: str , **kwgs) -> openeo.BatchJob:\n",
    "    \"\"\"Start a new job using the specified row out of the jobs GeoDataFrame and connection.\"\"\"\n",
    "    \n",
    "    # Get the variable parameters from the dataframe\n",
    "    tileID = row[\"tileID\"]\n",
    "    start = row[\"start_date\"]\n",
    "    end = row[\"end_date\"]\n",
    "    file_prefix = os.path.normpath(row[\"file_prefix\"])\n",
    "    processing_extent = string_to_dict(row[\"bbox\"])\n",
    "    \n",
    "    # get the openEO bbox dictionary for this tileID of processing block\n",
    "    #processing_extent = laea20km_id_to_extent(tileID)\n",
    "    \n",
    "    # define job_options, processing_options,  and collection_options\n",
    "    job_options = get_job_options(provider=provider)\n",
    "    #job_options['tile_grid']= 'utm-20km'\n",
    "    collection_options = get_collection_options(provider=provider)\n",
    "    processing_options = get_standard_processing_options(provider=provider, task='feature_generation')\n",
    "    \n",
    "    # add this job option to run it on old openEO version for CDSE server side\n",
    "    #job_options.update({\"image-name\": \"registry.prod.warsaw.openeo.dataspace.copernicus.eu/prod/openeo-geotrellis-kube:20240905-1881\"})\n",
    "    if not pd.isna(row[\"organization_id\"]):\n",
    "        job_options.update({'etl_organization_id':row[\"organization_id\"].astype(str)})\n",
    "\n",
    "    # This processing option add a filter on the S2 input data\n",
    "    if not pd.isna(row[\"s2_tileid_list\"]):\n",
    "        s2_tileid_list = convert_to_list(row[\"s2_tileid_list\"])\n",
    "        processing_options.update({\"s2_tileid_list\":s2_tileid_list})\n",
    "        \n",
    "    # define the progress graph\n",
    "    data_cube = generate_master_feature_cube(connection,\n",
    "                                             processing_extent,\n",
    "                                             start,\n",
    "                                             end,\n",
    "                                             **collection_options,\n",
    "                                             **processing_options)\n",
    "    saved_cube = data_cube.save_result('GTiff', options={\n",
    "                               'filename_prefix':file_prefix})#,'separate_asset_per_band':True})\n",
    "\n",
    "    if not pd.isna(row[\"s3_prefix\"]):\n",
    "        print(\"smt wrong\")\n",
    "        data_cube_workspace = saved_cube.export_workspace(\n",
    "            workspace =  row[\"export_workspace\"],\n",
    "            merge = row[\"s3_prefix\"]  # this determines the folder structure on the s3 bucket\n",
    "        )\n",
    "    else: data_cube_workspace = saved_cube\n",
    "    \n",
    "    job = data_cube_workspace.create_job(title=file_prefix, \n",
    "                               description=f'generation of full feature data cube of EO data for tile {tileID}', \n",
    "                               job_options=job_options)\n",
    "\n",
    "\n",
    "\n",
    "    return job"
   ],
   "id": "727446a1e451f481"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### run the jobs\n",
    "With our jobs database and job definition set up, we can now run the jobs using the `MultiBackendJobManager`. This involves defining a path to where we will store the job tracker which contains the job statuses and metadata.\n",
    "<br> <br>\n",
    "Note: If the specified job tracker path, points to an existing jobs database (either a .csv or .parquet file), that file will be used as the job tracker, and the input jobs database will be disregarded."
   ],
   "id": "d72a370fe886e7d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# generate a unique name for the job tracker database (here: CSV file)\n",
    "job_tracker = os.path.normpath(os.path.join(out_root, 'job_tracker.csv'))"
   ],
   "id": "f0036cc9f73703ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# init of MultiBackendJobManager (A viz switch has been added to enable vizualization of status\n",
    "manager = WeedJobManager(root_dir=os.path.normpath(out_root), storage_options = storage_options , poll_sleep=60, viz=False)"
   ],
   "id": "3546a87d5674da8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# create a connection to backend and add this active backend to the jobmanager\n",
    "backend = 'cdse'  # also possible: cdse, terrascope, development, creodias, sentinelhub\n",
    "connection = openeo.connect(url=\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n",
    "manager.add_backend(backend, connection=connection, parallel_jobs=2)"
   ],
   "id": "44c88bc2f4b7be40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# run the jobs stand-alone which blocks further usage of the Notebook up to all jobs are done\n",
    "#manager.run_jobs(df=jobs_database, start_job=start_job, job_db=job_tracker)"
   ],
   "id": "55e3ad0e39820600"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "##### better option in a Notebook is to use an extra thread to run the jobs and keep the notebook active\n",
    "# for that we have to convert our jobs_database into a JobDatabaseInterface which is saved in the job_tracker location\n",
    "# Note: we make sure if the job tracker database is already existing that this one is used and continue the processing\n",
    "job_db = CsvJobDatabase(path=job_tracker)\n",
    "if job_db.exists():\n",
    "    print(f\"Resuming `run_jobs` from existing {job_db}\")\n",
    "else:\n",
    "    df = manager._normalize_df(jobs_database)\n",
    "    job_db.persist(df)\n",
    "manager.start_job_thread(start_job=start_job, job_db=job_db)"
   ],
   "id": "b7f6a5e28ed002f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ea55fef50723cb0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1859450ce52af933"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weed",
   "language": "python",
   "name": "weed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
