{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fc2e44-2121-4ff2-a2f5-1c36a4f69d8f",
   "metadata": {},
   "source": [
    "# WEED Inference\n",
    "This notebook demonstrates how we integrate Earth Observation (EO) data processing with ONNX-based machine learning (ML) inference within the WEED framework.\n",
    "\n",
    "Overview of the Process\n",
    "1) Loading Data Cube:\n",
    "We start by loading a data cube that contains all the enabled training features as individual bands. This lazy loading approach ensures efficient memory usage during data processing.\n",
    "\n",
    "2) Model Metadata Retrieval:\n",
    "The models are stored in an openEO-accessible storage site. Each model contains metadata specifying the features it was trained on.\n",
    "\n",
    "Important Note: Users must add this metadata to the stored models.\n",
    "The onnx_model_utilities module provides a utility function to extract metadata from a JSON file and embed it into the ONNX model. This approach is temporary and will be replaced as model training becomes fully integrated within the WEED framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8243ee19-d745-4958-97a4-6e67075953ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import openeo\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(os.path.abspath('C:/Git_projects/eo_processing/src'))\n",
    "\n",
    "from eo_processing.utils.helper import init_connection, getUDFpath\n",
    "from eo_processing.utils.onnx_model_utilities import get_training_features_from_model\n",
    "from pathlib import Path\n",
    "\n",
    "from eo_processing.openeo.processing import generate_master_feature_cube\n",
    "from eo_processing.config import get_job_options, get_collection_options,  get_standard_processing_options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b125488-ec56-4954-b44a-8dda843e7f12",
   "metadata": {},
   "source": [
    "## Step 1: Connect to openEO Processing Backend\n",
    "To begin, we establish a connection to the openEO backend. This connection is essential for accessing and processing EO data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d33644bc-07e4-4733-841e-761b6772b0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated using refresh token.\n"
     ]
    }
   ],
   "source": [
    "backend = 'cdse' \n",
    "# establish the connection to the selected backend\n",
    "connection = init_connection(backend)\n",
    "collection_options = get_collection_options(provider=backend)\n",
    "\n",
    "# We call again the standard processing options for feature generation\n",
    "processing_options = get_standard_processing_options(provider=backend, task='feature_generation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae12555-8963-4510-af03-73e89ecaae60",
   "metadata": {},
   "source": [
    "## Step 2: Specify Space and Time Context\n",
    "Define the spatial and temporal parameters for the data cube. This step ensures that only relevant data is loaded and processed during inference.\n",
    "\n",
    "Important note, the job options were seme-optimised in function of the input size. Larger extents, will have higher memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e1c84a6-9aa2-4efc-aa49-fa59782731a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the time context is given by start and end date\n",
    "start = '2021-01-01'\n",
    "end = '2021-02-01'   # the end is always exclusive\n",
    "AOI = {\n",
    "    'east': 4841500,\n",
    "    'south': 2808500,\n",
    "    'west': 4831500,\n",
    "    'north': 2818500,\n",
    "    'crs': 'EPSG:3035'\n",
    "}\n",
    "\n",
    "#note these job options were optimised for the spatiotemporal extent given above\n",
    "\n",
    "job_options = {'driver-memory': '500m',\n",
    " 'driver-memoryOverhead': '1000m',\n",
    "\n",
    " 'executor-memory': '1000m',\n",
    " 'executor-memoryOverhead': '500m',\n",
    " 'python-memory': '3000m',\n",
    " }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e876841-8a9e-4d2d-a45f-4ca240643534",
   "metadata": {},
   "source": [
    "### Step 3: Parse the available models\n",
    "We extract all model names available in the model folder of interest. This folder should be on a publically available repository such that openEO can download the models later on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa683328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Load the JSON data from the file\n",
    "MODEL_URL = \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/\"\n",
    "\n",
    "# Replace with your CloudFerro S3 endpoint and credentials\n",
    "s3_endpoint = 'https://s3.waw3-1.cloudferro.com'\n",
    "access_key = 'yy'\n",
    "secret_key = 'xx'\n",
    "bucket_name = 'ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o' \n",
    "s3_directory = 'models'  # Path in the S3 bucket\n",
    "\n",
    "session = boto3.session.Session()\n",
    "s3_client = session.client('s3',\n",
    "                            endpoint_url=s3_endpoint,\n",
    "                            aws_access_key_id=access_key,\n",
    "                            aws_secret_access_key=secret_key)\n",
    "\n",
    "# List objects in the specified directory\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=s3_directory)\n",
    "\n",
    "# Filter and create a list of .onnx file names\n",
    "onnx_files = []\n",
    "if 'Contents' in response:\n",
    "    MODELS_NAMES = [obj['Key'].split('/')[-1] for obj in response['Contents'] if obj['Key'].endswith('.onnx')]\n",
    "MODELS_NAMES = MODELS_NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3fabf8",
   "metadata": {},
   "source": [
    "### Step 4: Define the input cube\n",
    "\n",
    "Here we create the input datacube for the ML network inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66e5eb29-0bf2-4eab-9128-33786a011de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we link towards the used model\n",
    "\n",
    "#create the progress graph for the feature cube\n",
    "# define the S1/S2 processed feature cube (Note: do not set spatial extent since we had it over in the end)\n",
    "data_cube = generate_master_feature_cube(connection,\n",
    "                                            AOI,\n",
    "                                            start,\n",
    "                                            end,\n",
    "                                            **collection_options,\n",
    "                                            **processing_options)\n",
    "\n",
    "# now we merge in the NON ON-DEMAND processed features (DEM and WENR features)\n",
    "# load the DEM from a CDSE collection\n",
    "DEM = connection.load_collection(\n",
    "    \"COPERNICUS_30\",\n",
    "    bands=[\"DEM\"])\n",
    "# reduce the temporal domain since copernicus_30 collection is \"special\" and feature only are one time stamp\n",
    "DEM = DEM.reduce_dimension(dimension='t', reducer=lambda x: x.last(ignore_nodata=True))\n",
    "# resample the cube to 10m and EPSG of corresponding 20x20km grid tile\n",
    "DEM = DEM.resample_spatial(projection=processing_options['target_crs'],\n",
    "                            resolution=processing_options['resolution'],\n",
    "                            method=\"bilinear\").filter_bbox(AOI)\n",
    "# merge into the S1/S2 data cube\n",
    "data_cube = data_cube.merge_cubes(DEM)\n",
    "\n",
    "# load the WERN features from public STAC\n",
    "WENR = connection.load_stac(\"https://stac.openeo.vito.be/collections/wenr_features\")\n",
    "# resample the cube to 10m and EPSG of corresponding 20x20km grid tile\n",
    "WENR = WENR.resample_spatial(projection=processing_options['target_crs'],\n",
    "                                resolution=processing_options['resolution'],\n",
    "                                method=\"near\").filter_bbox(AOI)\n",
    "# drop the time dimension\n",
    "try:\n",
    "    WENR = WENR.drop_dimension('t')\n",
    "except:\n",
    "    # workaround if we still have the client issues with the time dimensions for STAC dataset with only one time stamp\n",
    "    WENR.metadata = WENR.metadata.add_dimension(\"t\", label=None, type=\"temporal\")\n",
    "    WENR = WENR.drop_dimension('t')\n",
    "# merge into the S1/S2 data cube\n",
    "data_cube = data_cube.merge_cubes(WENR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8291ee68",
   "metadata": {},
   "source": [
    "## Step 5: Multimodel inference.\n",
    "\n",
    "Here we create and start an openeo job which performs the inference of all onnx models in the provided folder. \n",
    "\n",
    "As a next step we should eveluate wheter we prefer 1 logn running job over multiple parallel openEO jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "314e30d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level1_class-0_129predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level1_class-0_129predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-C_71predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level2_class-C_71predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-D_68predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level2_class-D_68predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-E_85predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level2_class-E_85predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-F_90predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level2_class-F_90predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-G_164predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level2_class-G_164predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-H_65predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level2_class-H_65predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-I_50predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level2_class-I_50predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-J_62predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level2_class-J_62predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-X_54predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level2_class-X_54predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-C1_62predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level3_class-C1_62predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-C3_62predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level3_class-C3_62predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-E1_50predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level3_class-E1_50predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-E2_50predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level3_class-E2_50predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-E3_50predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level3_class-E3_50predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-E4_50predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level3_class-E4_50predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-E5_50predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level3_class-E5_50predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-F2_60predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level3_class-F2_60predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-F3_50predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level3_class-F3_50predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-G1_74predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level3_class-G1_74predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-G3_98predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level3_class-G3_98predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-H2_50predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level3_class-H2_50predictors_v1.onnx\n",
      "Downloading file from https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-H3_50predictors_v1.onnx...\n",
      "Loaded ONNX model from /tmp/cache\\Level3_class-H3_50predictors_v1.onnx\n",
      "0:00:00 Job 'j-241212f33620454ba9da2fb2b3b49638': send 'start'\n",
      "0:01:13 Job 'j-241212f33620454ba9da2fb2b3b49638': created (progress 0%)\n",
      "0:01:18 Job 'j-241212f33620454ba9da2fb2b3b49638': created (progress 0%)\n",
      "0:01:25 Job 'j-241212f33620454ba9da2fb2b3b49638': created (progress 0%)\n",
      "0:01:33 Job 'j-241212f33620454ba9da2fb2b3b49638': created (progress 0%)\n",
      "0:01:43 Job 'j-241212f33620454ba9da2fb2b3b49638': created (progress 0%)\n",
      "0:01:56 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:02:12 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:02:31 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:02:55 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:03:26 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:04:03 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:04:50 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:05:49 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:06:49 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:07:50 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:08:51 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:09:51 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:10:52 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:11:52 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:12:52 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:13:53 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:14:53 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:15:54 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:16:54 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:17:55 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:18:55 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:19:55 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:20:56 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:21:56 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:22:57 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:23:57 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:24:58 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:25:58 Job 'j-241212f33620454ba9da2fb2b3b49638': running (progress N/A)\n",
      "0:26:59 Job 'j-241212f33620454ba9da2fb2b3b49638': finished (progress 100%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <script>\n",
       "    if (!window.customElements || !window.customElements.get('openeo-job')) {\n",
       "        var el = document.createElement('script');\n",
       "        el.src = \"https://cdn.jsdelivr.net/npm/@openeo/vue-components@2/assets/openeo.min.js\";\n",
       "        document.head.appendChild(el);\n",
       "\n",
       "        var font = document.createElement('font');\n",
       "        font.as = \"font\";\n",
       "        font.type = \"font/woff2\";\n",
       "        font.crossOrigin = true;\n",
       "        font.href = \"https://use.fontawesome.com/releases/v5.13.0/webfonts/fa-solid-900.woff2\"\n",
       "        document.head.appendChild(font);\n",
       "    }\n",
       "    </script>\n",
       "    <openeo-job>\n",
       "        <script type=\"application/json\">{\"currency\": \"credits\", \"job\": {\"costs\": 29, \"created\": \"2024-12-12T14:00:23Z\", \"id\": \"j-241212f33620454ba9da2fb2b3b49638\", \"process\": {\"process_graph\": {\"aggregatetemporalperiod1\": {\"arguments\": {\"data\": {\"from_node\": \"mask1\"}, \"period\": \"dekad\", \"reducer\": {\"process_graph\": {\"median1\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}}, \"process_id\": \"median\", \"result\": true}}}}, \"process_id\": \"aggregate_temporal_period\"}, \"aggregatetemporalperiod2\": {\"arguments\": {\"data\": {\"from_node\": \"resamplespatial2\"}, \"period\": \"dekad\", \"reducer\": {\"process_graph\": {\"mean1\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}}, \"process_id\": \"mean\", \"result\": true}}}}, \"process_id\": \"aggregate_temporal_period\"}, \"apply1\": {\"arguments\": {\"data\": {\"from_node\": \"aggregatetemporalperiod1\"}, \"process\": {\"process_graph\": {\"linearscalerange1\": {\"arguments\": {\"inputMax\": 65534, \"inputMin\": 0, \"outputMax\": 65534, \"outputMin\": 0, \"x\": {\"from_parameter\": \"x\"}}, \"process_id\": \"linear_scale_range\", \"result\": true}}}}, \"process_id\": \"apply\"}, \"apply2\": {\"arguments\": {\"data\": {\"from_node\": \"apply1\"}, \"process\": {\"process_graph\": {\"linearscalerange2\": {\"arguments\": {\"inputMax\": 10000, \"inputMin\": 0, \"outputMax\": 1, \"outputMin\": 0, \"x\": {\"from_parameter\": \"x\"}}, \"process_id\": \"linear_scale_range\", \"result\": true}}}}, \"process_id\": \"apply\"}, \"apply3\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension2\"}, \"process\": {\"process_graph\": {\"linearscalerange3\": {\"arguments\": {\"inputMax\": 65534, \"inputMin\": 1, \"outputMax\": 65534, \"outputMin\": 1, \"x\": {\"from_parameter\": \"x\"}}, \"process_id\": \"linear_scale_range\", \"result\": true}}}}, \"process_id\": \"apply\"}, \"apply4\": {\"arguments\": {\"data\": {\"from_node\": \"mergecubes25\"}, \"process\": {\"process_graph\": {\"linearscalerange4\": {\"arguments\": {\"inputMax\": 100, \"inputMin\": 0, \"outputMax\": 100, \"outputMin\": 0, \"x\": {\"from_parameter\": \"x\"}}, \"process_id\": \"linear_scale_range\", \"result\": true}}}}, \"process_id\": \"apply\"}, \"applydimension1\": {\"arguments\": {\"data\": {\"from_node\": \"apply2\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"add1\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement1\"}, \"y\": {\"from_node\": \"arrayelement2\"}}, \"process_id\": \"add\"}, \"add10\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement5\"}, \"y\": {\"from_node\": \"arrayelement4\"}}, \"process_id\": \"add\"}, \"add11\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement1\"}, \"y\": {\"from_node\": \"arrayelement2\"}}, \"process_id\": \"add\"}, \"add12\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement5\"}, \"y\": {\"from_node\": \"arrayelement4\"}}, \"process_id\": \"add\"}, \"add13\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement3\"}, \"y\": {\"from_node\": \"arrayelement6\"}}, \"process_id\": \"add\"}, \"add14\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement7\"}, \"y\": {\"from_node\": \"arrayelement2\"}}, \"process_id\": \"add\"}, \"add15\": {\"arguments\": {\"x\": 705, \"y\": {\"from_node\": \"multiply4\"}}, \"process_id\": \"add\"}, \"add2\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement1\"}, \"y\": {\"from_node\": \"arrayelement2\"}}, \"process_id\": \"add\"}, \"add3\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement1\"}, \"y\": {\"from_node\": \"arrayelement4\"}}, \"process_id\": \"add\"}, \"add4\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement5\"}, \"y\": {\"from_node\": \"arrayelement1\"}}, \"process_id\": \"add\"}, \"add5\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement5\"}, \"y\": {\"from_node\": \"arrayelement2\"}}, \"process_id\": \"add\"}, \"add6\": {\"arguments\": {\"x\": {\"from_node\": \"add5\"}, \"y\": {\"from_node\": \"arrayelement6\"}}, \"process_id\": \"add\"}, \"add7\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement5\"}, \"y\": {\"from_node\": \"arrayelement2\"}}, \"process_id\": \"add\"}, \"add8\": {\"arguments\": {\"x\": {\"from_node\": \"add7\"}, \"y\": {\"from_node\": \"arrayelement6\"}}, \"process_id\": \"add\"}, \"add9\": {\"arguments\": {\"x\": {\"from_node\": \"divide7\"}, \"y\": {\"from_node\": \"arrayelement4\"}}, \"process_id\": \"add\"}, \"arrayelement1\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"index\": 6}, \"process_id\": \"array_element\"}, \"arrayelement2\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"index\": 2}, \"process_id\": \"array_element\"}, \"arrayelement3\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"index\": 3}, \"process_id\": \"array_element\"}, \"arrayelement4\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"index\": 8}, \"process_id\": \"array_element\"}, \"arrayelement5\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"index\": 1}, \"process_id\": \"array_element\"}, \"arrayelement6\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"index\": 9}, \"process_id\": \"array_element\"}, \"arrayelement7\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"index\": 5}, \"process_id\": \"array_element\"}, \"arrayelement8\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"index\": 4}, \"process_id\": \"array_element\"}, \"arraymodify1\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"index\": 10, \"values\": [{\"from_node\": \"divide1\"}, {\"from_node\": \"power1\"}, {\"from_node\": \"subtract4\"}, {\"from_node\": \"multiply3\"}, {\"from_node\": \"divide4\"}, {\"from_node\": \"divide5\"}, {\"from_node\": \"divide8\"}, {\"from_node\": \"divide9\"}, {\"from_node\": \"subtract12\"}, {\"from_node\": \"divide12\"}, {\"from_node\": \"add15\"}, {\"from_node\": \"divide16\"}]}, \"process_id\": \"array_modify\", \"result\": true}, \"divide1\": {\"arguments\": {\"x\": {\"from_node\": \"subtract1\"}, \"y\": {\"from_node\": \"add1\"}}, \"process_id\": \"divide\"}, \"divide10\": {\"arguments\": {\"x\": {\"from_node\": \"subtract10\"}, \"y\": {\"from_node\": \"add11\"}}, \"process_id\": \"divide\"}, \"divide11\": {\"arguments\": {\"x\": {\"from_node\": \"subtract11\"}, \"y\": {\"from_node\": \"add12\"}}, \"process_id\": \"divide\"}, \"divide12\": {\"arguments\": {\"x\": {\"from_node\": \"subtract13\"}, \"y\": {\"from_node\": \"add13\"}}, \"process_id\": \"divide\"}, \"divide13\": {\"arguments\": {\"x\": {\"from_node\": \"add14\"}, \"y\": 2}, \"process_id\": \"divide\"}, \"divide14\": {\"arguments\": {\"x\": {\"from_node\": \"subtract14\"}, \"y\": {\"from_node\": \"subtract15\"}}, \"process_id\": \"divide\"}, \"divide15\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement3\"}, \"y\": {\"from_node\": \"arrayelement8\"}}, \"process_id\": \"divide\"}, \"divide16\": {\"arguments\": {\"x\": {\"from_node\": \"subtract16\"}, \"y\": {\"from_node\": \"divide15\"}}, \"process_id\": \"divide\"}, \"divide2\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement1\"}, \"y\": {\"from_node\": \"arrayelement3\"}}, \"process_id\": \"divide\"}, \"divide3\": {\"arguments\": {\"x\": {\"from_node\": \"subtract5\"}, \"y\": {\"from_node\": \"add2\"}}, \"process_id\": \"divide\"}, \"divide4\": {\"arguments\": {\"x\": {\"from_node\": \"subtract6\"}, \"y\": {\"from_node\": \"add3\"}}, \"process_id\": \"divide\"}, \"divide5\": {\"arguments\": {\"x\": {\"from_node\": \"subtract7\"}, \"y\": {\"from_node\": \"add4\"}}, \"process_id\": \"divide\"}, \"divide6\": {\"arguments\": {\"x\": {\"from_node\": \"add6\"}, \"y\": 3}, \"process_id\": \"divide\"}, \"divide7\": {\"arguments\": {\"x\": {\"from_node\": \"add8\"}, \"y\": 3}, \"process_id\": \"divide\"}, \"divide8\": {\"arguments\": {\"x\": {\"from_node\": \"subtract8\"}, \"y\": {\"from_node\": \"add9\"}}, \"process_id\": \"divide\"}, \"divide9\": {\"arguments\": {\"x\": {\"from_node\": \"subtract9\"}, \"y\": {\"from_node\": \"add10\"}}, \"process_id\": \"divide\"}, \"multiply1\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement1\"}, \"y\": {\"from_node\": \"subtract2\"}}, \"process_id\": \"multiply\"}, \"multiply2\": {\"arguments\": {\"x\": {\"from_node\": \"multiply1\"}, \"y\": {\"from_node\": \"subtract3\"}}, \"process_id\": \"multiply\"}, \"multiply3\": {\"arguments\": {\"x\": {\"from_node\": \"divide3\"}, \"y\": {\"from_node\": \"arrayelement1\"}}, \"process_id\": \"multiply\"}, \"multiply4\": {\"arguments\": {\"x\": 35, \"y\": {\"from_node\": \"divide14\"}}, \"process_id\": \"multiply\"}, \"power1\": {\"arguments\": {\"base\": {\"from_node\": \"multiply2\"}, \"p\": 0.3333333333333333}, \"process_id\": \"power\"}, \"subtract1\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement1\"}, \"y\": {\"from_node\": \"arrayelement2\"}}, \"process_id\": \"subtract\"}, \"subtract10\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement1\"}, \"y\": {\"from_node\": \"arrayelement2\"}}, \"process_id\": \"subtract\"}, \"subtract11\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement5\"}, \"y\": {\"from_node\": \"arrayelement4\"}}, \"process_id\": \"subtract\"}, \"subtract12\": {\"arguments\": {\"x\": {\"from_node\": \"divide10\"}, \"y\": {\"from_node\": \"divide11\"}}, \"process_id\": \"subtract\"}, \"subtract13\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement3\"}, \"y\": {\"from_node\": \"arrayelement6\"}}, \"process_id\": \"subtract\"}, \"subtract14\": {\"arguments\": {\"x\": {\"from_node\": \"divide13\"}, \"y\": {\"from_node\": \"arrayelement3\"}}, \"process_id\": \"subtract\"}, \"subtract15\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement8\"}, \"y\": {\"from_node\": \"arrayelement3\"}}, \"process_id\": \"subtract\"}, \"subtract16\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement7\"}, \"y\": {\"from_node\": \"arrayelement2\"}}, \"process_id\": \"subtract\"}, \"subtract2\": {\"arguments\": {\"x\": 1, \"y\": {\"from_node\": \"arrayelement2\"}}, \"process_id\": \"subtract\"}, \"subtract3\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement1\"}, \"y\": {\"from_node\": \"arrayelement2\"}}, \"process_id\": \"subtract\"}, \"subtract4\": {\"arguments\": {\"x\": {\"from_node\": \"divide2\"}, \"y\": 1}, \"process_id\": \"subtract\"}, \"subtract5\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement1\"}, \"y\": {\"from_node\": \"arrayelement2\"}}, \"process_id\": \"subtract\"}, \"subtract6\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement1\"}, \"y\": {\"from_node\": \"arrayelement4\"}}, \"process_id\": \"subtract\"}, \"subtract7\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement5\"}, \"y\": {\"from_node\": \"arrayelement1\"}}, \"process_id\": \"subtract\"}, \"subtract8\": {\"arguments\": {\"x\": {\"from_node\": \"divide6\"}, \"y\": {\"from_node\": \"arrayelement4\"}}, \"process_id\": \"subtract\"}, \"subtract9\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement5\"}, \"y\": {\"from_node\": \"arrayelement4\"}}, \"process_id\": \"subtract\"}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension10\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands6\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf5\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-F_90predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension11\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands7\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf6\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-G_164predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension12\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands8\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf7\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-H_65predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension13\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands9\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf8\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-I_50predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension14\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands10\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf9\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-J_62predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension15\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands11\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf10\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-X_54predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension16\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands12\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf11\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-C1_62predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension17\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands13\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf12\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-C3_62predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension18\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands14\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf13\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-E1_50predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension19\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands15\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf14\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-E2_50predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension2\": {\"arguments\": {\"data\": {\"from_node\": \"aggregatetemporalperiod2\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"add16\": {\"arguments\": {\"x\": {\"from_node\": \"multiply5\"}, \"y\": 83}, \"process_id\": \"add\"}, \"add17\": {\"arguments\": {\"x\": {\"from_node\": \"multiply6\"}, \"y\": 83}, \"process_id\": \"add\"}, \"arraycreate1\": {\"arguments\": {\"data\": [{\"from_node\": \"if1\"}, {\"from_node\": \"if2\"}]}, \"process_id\": \"array_create\", \"result\": true}, \"arrayelement10\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"index\": 0}, \"process_id\": \"array_element\"}, \"arrayelement11\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"index\": 1}, \"process_id\": \"array_element\"}, \"arrayelement12\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"index\": 1}, \"process_id\": \"array_element\"}, \"arrayelement9\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"index\": 0}, \"process_id\": \"array_element\"}, \"divide17\": {\"arguments\": {\"x\": {\"from_node\": \"add16\"}, \"y\": 20}, \"process_id\": \"divide\"}, \"divide18\": {\"arguments\": {\"x\": {\"from_node\": \"add17\"}, \"y\": 20}, \"process_id\": \"divide\"}, \"if1\": {\"arguments\": {\"accept\": 1, \"reject\": {\"from_node\": \"power2\"}, \"value\": {\"from_node\": \"isnodata1\"}}, \"process_id\": \"if\"}, \"if2\": {\"arguments\": {\"accept\": 1, \"reject\": {\"from_node\": \"power3\"}, \"value\": {\"from_node\": \"isnodata2\"}}, \"process_id\": \"if\"}, \"isnodata1\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement10\"}}, \"process_id\": \"is_nodata\"}, \"isnodata2\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement12\"}}, \"process_id\": \"is_nodata\"}, \"log1\": {\"arguments\": {\"base\": 10, \"x\": {\"from_node\": \"arrayelement9\"}}, \"process_id\": \"log\"}, \"log2\": {\"arguments\": {\"base\": 10, \"x\": {\"from_node\": \"arrayelement11\"}}, \"process_id\": \"log\"}, \"multiply5\": {\"arguments\": {\"x\": 10, \"y\": {\"from_node\": \"log1\"}}, \"process_id\": \"multiply\"}, \"multiply6\": {\"arguments\": {\"x\": 10, \"y\": {\"from_node\": \"log2\"}}, \"process_id\": \"multiply\"}, \"power2\": {\"arguments\": {\"base\": 10, \"p\": {\"from_node\": \"divide17\"}}, \"process_id\": \"power\"}, \"power3\": {\"arguments\": {\"base\": 10, \"p\": {\"from_node\": \"divide18\"}}, \"process_id\": \"power\"}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension20\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands16\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf15\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-E3_50predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension21\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands17\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf16\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-E4_50predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension22\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands18\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf17\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-E5_50predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension23\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands19\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf18\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-F2_60predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension24\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands20\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf19\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-F3_50predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension25\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands21\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf20\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-G1_74predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension26\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands22\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf21\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-G3_98predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension27\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands23\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf22\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-H2_50predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension28\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands24\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf23\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level3_class-H3_50predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension3\": {\"arguments\": {\"data\": {\"from_node\": \"apply3\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"arraycreate2\": {\"arguments\": {\"data\": [{\"from_node\": \"subtract17\"}, {\"from_node\": \"subtract18\"}]}, \"process_id\": \"array_create\", \"result\": true}, \"arrayelement13\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"index\": 0}, \"process_id\": \"array_element\"}, \"arrayelement14\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"index\": 1}, \"process_id\": \"array_element\"}, \"log3\": {\"arguments\": {\"base\": 10, \"x\": {\"from_node\": \"arrayelement13\"}}, \"process_id\": \"log\"}, \"log4\": {\"arguments\": {\"base\": 10, \"x\": {\"from_node\": \"arrayelement14\"}}, \"process_id\": \"log\"}, \"multiply7\": {\"arguments\": {\"x\": 20, \"y\": {\"from_node\": \"log3\"}}, \"process_id\": \"multiply\"}, \"multiply8\": {\"arguments\": {\"x\": 20, \"y\": {\"from_node\": \"log4\"}}, \"process_id\": \"multiply\"}, \"subtract17\": {\"arguments\": {\"x\": {\"from_node\": \"multiply7\"}, \"y\": 83}, \"process_id\": \"subtract\"}, \"subtract18\": {\"arguments\": {\"x\": {\"from_node\": \"multiply8\"}, \"y\": 83}, \"process_id\": \"subtract\"}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension4\": {\"arguments\": {\"context\": {\"TileSize\": 128, \"parallel\": true}, \"data\": {\"from_node\": \"applydimension3\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"add18\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement15\"}, \"y\": {\"from_node\": \"arrayelement16\"}}, \"process_id\": \"add\"}, \"arraycreate3\": {\"arguments\": {\"data\": [{\"from_node\": \"arrayelement15\"}, {\"from_node\": \"arrayelement16\"}, {\"from_node\": \"divide19\"}, {\"from_node\": \"subtract19\"}, {\"from_node\": \"divide20\"}]}, \"process_id\": \"array_create\", \"result\": true}, \"arrayelement15\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"label\": \"VV\"}, \"process_id\": \"array_element\"}, \"arrayelement16\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"label\": \"VH\"}, \"process_id\": \"array_element\"}, \"divide19\": {\"arguments\": {\"x\": {\"from_node\": \"multiply9\"}, \"y\": {\"from_node\": \"add18\"}}, \"process_id\": \"divide\"}, \"divide20\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement16\"}, \"y\": {\"from_node\": \"arrayelement15\"}}, \"process_id\": \"divide\"}, \"multiply9\": {\"arguments\": {\"x\": 4, \"y\": {\"from_node\": \"arrayelement16\"}}, \"process_id\": \"multiply\"}, \"subtract19\": {\"arguments\": {\"x\": {\"from_node\": \"arrayelement16\"}, \"y\": {\"from_node\": \"arrayelement15\"}}, \"process_id\": \"subtract\"}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension5\": {\"arguments\": {\"context\": {\"TileSize\": 128, \"parallel\": true}, \"data\": {\"from_node\": \"mergecubes1\"}, \"dimension\": \"t\", \"process\": {\"process_graph\": {\"arrayconcat1\": {\"arguments\": {\"array1\": {\"from_node\": \"quantiles1\"}, \"array2\": [{\"from_node\": \"mean2\"}, {\"from_node\": \"sd1\"}, {\"from_node\": \"sum1\"}, {\"from_node\": \"subtract20\"}]}, \"process_id\": \"array_concat\", \"result\": true}, \"mean2\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}}, \"process_id\": \"mean\"}, \"quantiles1\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"probabilities\": [0.02, 0.25, 0.5, 0.75, 0.98]}, \"process_id\": \"quantiles\"}, \"quantiles2\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"probabilities\": [0.75]}, \"process_id\": \"quantiles\"}, \"quantiles3\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"probabilities\": [0.25]}, \"process_id\": \"quantiles\"}, \"sd1\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}}, \"process_id\": \"sd\"}, \"subtract20\": {\"arguments\": {\"x\": {\"from_node\": \"quantiles2\"}, \"y\": {\"from_node\": \"quantiles3\"}}, \"process_id\": \"subtract\"}, \"sum1\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}}, \"process_id\": \"sum\"}}}, \"target_dimension\": \"bands\"}, \"process_id\": \"apply_dimension\"}, \"applydimension6\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands2\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf1\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level1_class-0_129predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension7\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands3\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf2\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-C_71predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension8\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands4\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf3\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-D_68predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applydimension9\": {\"arguments\": {\"data\": {\"from_node\": \"filterbands5\"}, \"dimension\": \"bands\", \"process\": {\"process_graph\": {\"runudf4\": {\"arguments\": {\"context\": {\"model_url\": \"https://s3.waw3-1.cloudferro.com/swift/v1/ecdc-waw3-1-ekqouvq3otv8hmw0njzuvo0g4dy0ys8r985n7dggjis3erkpn5o/models/Level2_class-E_85predictors_v1.onnx\"}, \"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"# /// script\\n# dependencies = [\\n# \\\"filelock\\\",\\n# \\\"onnxruntime\\\",\\n# ]\\n# ///\\n\\nimport os\\nimport functools\\nimport requests\\nimport tempfile\\nimport onnxruntime as ort\\nimport xarray as xr\\nimport numpy as np\\nimport shutil\\nfrom urllib.parse import urlparse\\nfrom openeo.udf import inspect\\nfrom typing import Dict\\nfrom filelock import FileLock\\n\\n\\ndef is_zip_file(url: str) -> bool:\\n    \\\"\\\"\\\"Check if the URL points to a ZIP file.\\\"\\\"\\\"\\n    return url.lower().endswith('.zip')\\n\\ndef is_onnx_file(file_path: str) -> bool:\\n    \\\"\\\"\\\"Check if the file is an ONNX model based on its extension.\\\"\\\"\\\"\\n    return file_path.endswith('.onnx')\\n    \\ndef download_file_with_lock(url: str, max_file_size_mb: int = 100, cache_dir: str = '/tmp/cache') -> str:\\n    \\\"\\\"\\\"Download a file with concurrency protection and store it temporarily.\\\"\\\"\\\"\\n    \\n    # Extract the file name from the URL (e.g., \\\"model_1.onnx\\\")\\n    file_name = os.path.basename(urlparse(url).path)\\n    \\n    # Construct the file path within the cache directory (e.g., '/tmp/cache/model.onnx')\\n    file_path = os.path.join(cache_dir, file_name)\\n    \\n    # Lock file to prevent concurrent downloads\\n    lock_path = file_path + '.download.lock'\\n    lock = FileLock(lock_path)\\n    \\n    with lock:\\n        # Check if the file already exists in the cache\\n        if os.path.exists(file_path):\\n            print(f\\\"File {file_path} already exists in cache.\\\")\\n            return file_path\\n        \\n        try:\\n            # Download the file to a temporary location\\n            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\\\".onnx\\\")\\n            temp_file_path = temp_file.name  # Store the temporary file path\\n            \\n            inspect(message=f\\\"Downloading file from {url}...\\\")\\n            response = requests.get(url, stream=True)\\n            if response.status_code == 200:\\n                file_size = 0\\n                with temp_file:\\n                    for chunk in response.iter_content(chunk_size=1024):\\n                        temp_file.write(chunk)\\n                        file_size += len(chunk)\\n                        if file_size > max_file_size_mb * 1024 * 1024:\\n                            raise ValueError(f\\\"Downloaded file exceeds the size limit of {max_file_size_mb} MB\\\")\\n\\n                inspect(message=f\\\"Downloaded file to {temp_file_path}\\\")\\n                \\n                # After download is complete, move the file from temp to the final destination\\n                shutil.move(temp_file_path, file_path)  # Move the file to final location\\n\\n                return file_path  # Return path of the final model file\\n\\n            else:\\n                raise ValueError(f\\\"Failed to download file, status code: {response.status_code}\\\")\\n\\n        except Exception as e:\\n            if os.path.exists(temp_file_path):\\n                os.remove(temp_file_path)  # Clean up temporary file on error\\n            raise ValueError(f\\\"Error downloading file: {e}\\\")\\n\\n@functools.lru_cache(maxsize=5)\\ndef load_onnx_model(model_url: str, cache_dir: str = '/tmp/cache') -> ort.InferenceSession:\\n    \\\"\\\"\\\"\\n    Load an ONNX model into an ONNX Runtime session.\\n\\n    Args:\\n        model_url (str): The URL or file path to the ONNX model.\\n        cache_dir (str): Directory for caching or processing model files.\\n\\n    Returns:\\n        ort.InferenceSession: The ONNX Runtime session for the loaded model.\\n\\n    Raises:\\n        ValueError: If the model file cannot be processed or loaded.\\n    \\\"\\\"\\\"\\n    try:\\n        # Process the model file to ensure it's a valid ONNX model\\n        model_path = download_file_with_lock(model_url, cache_dir=cache_dir)\\n\\n        # Initialize the ONNX Runtime session\\n        inspect(message=f\\\"Initializing ONNX Runtime session for model at {model_path}...\\\")\\n        ort_session = ort.InferenceSession(model_path, providers=[\\\"CPUExecutionProvider\\\"])\\n        inspect(message=\\\"ONNX model successfully loaded into ONNX Runtime session.\\\")\\n        return ort_session\\n\\n    except Exception as e:\\n        raise ValueError(f\\\"Failed to load the ONNX model from {model_url}. Error: {e}\\\")\\n\\ndef preprocess_input(input_xr: xr.DataArray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Preprocess the input DataArray by ensuring the dimensions are in the correct order,\\n    reshaping it, and returning the reshaped numpy array and the original shape.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Preprocessing the input\\\")\\n    input_xr = input_xr.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    input_shape = input_xr.shape\\n    input_np = input_xr.values.reshape(-1, ort_session.get_inputs()[0].shape[1])\\n    return input_np, input_shape\\n\\ndef run_inference(input_np: np.ndarray, ort_session: ort.InferenceSession) -> tuple:\\n    \\\"\\\"\\\"\\n    Run inference using the ONNX runtime session and return predicted labels and probabilities.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Running inference\\\")\\n    ort_inputs = {ort_session.get_inputs()[0].name: input_np}\\n    ort_outputs = ort_session.run(None, ort_inputs)\\n    predicted_labels = ort_outputs[0]\\n    probabilities_dicts = ort_outputs[1]\\n    return predicted_labels, probabilities_dicts\\n\\ndef postprocess_output(predicted_labels: np.ndarray, probabilities_dicts: list, input_shape: tuple) -> tuple:\\n    \\\"\\\"\\\"\\n    Postprocess the output by reshaping the predicted labels and probabilities into the original spatial structure.\\n    \\\"\\\"\\\"\\n\\n    inspect(message=f\\\"Postprocessing the output\\\")\\n    class_labels = list(probabilities_dicts[0].keys())\\n\\n    # Convert probabilities into a 2D array\\n    probabilities = np.array([[prob[class_id] for class_id in class_labels] for prob in probabilities_dicts])\\n\\n    # Reshape to match the (y, x) spatial structure\\n    predicted_labels = predicted_labels.reshape(input_shape[0], input_shape[1])\\n    probabilities = probabilities.reshape(len(class_labels), input_shape[0], input_shape[1])\\n    probabilities = (probabilities / probabilities.sum(axis=0, keepdims=True)) * 100\\n\\n\\n    return predicted_labels, probabilities\\n\\ndef create_output_xarray(predicted_labels: np.ndarray, probabilities: np.ndarray, \\n                         input_xr: xr.DataArray) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Create an xarray DataArray with predicted labels and probabilities stacked along the bands dimension.\\n    \\\"\\\"\\\"\\n    inspect(message=f\\\"Ceating output xarray\\\")\\n    #combined_data = np.concatenate([\\n    #    predicted_labels[np.newaxis, :, :],  # Shape (1, y, x) for predicted labels\\n    #    probabilities  # Shape (n_classes, y, x) for probabilities\\n    #], axis=0)\\n\\n    return xr.DataArray(\\n        probabilities,\\n        dims=[\\\"bands\\\", \\\"y\\\", \\\"x\\\"],\\n        coords={\\n            'y': input_xr.coords['y'],\\n            'x': input_xr.coords['x']\\n        }\\n    )\\n\\ndef apply_model(input_xr: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Run inference on the given input data using the provided ONNX runtime session.\\n    This method is called for each timestep in the chunk received by apply_datacube.\\n    \\\"\\\"\\\"\\n    # Step 1: Load the ONNX model\\n    try:\\n        ort_session = load_onnx_model(context.get(\\\"model_url\\\"), cache_dir=\\\"/tmp/cache\\\")\\n    except ValueError as e:\\n        raise RuntimeError(f\\\"Model loading failed: {e}\\\")\\n\\n    # Step 2: Preprocess the input\\n    input_np, input_shape = preprocess_input(input_xr, ort_session)\\n\\n    # Step 3: Perform inference\\n    predicted_labels, probabilities_dicts = run_inference(input_np, ort_session)\\n\\n    # Step 4: Postprocess the output\\n    predicted_labels, probabilities = postprocess_output(predicted_labels, probabilities_dicts, input_shape)\\n\\n    # Step 5: Create the output xarray\\n    return create_output_xarray(predicted_labels, probabilities, input_xr)\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Function that is called for each chunk of data that is processed.\\n    The function name and arguments are defined by the UDF API.\\n    \\n    More information can be found here: \\n    https://open-eo.github.io/openeo-python-client/udf.html#udf-function-names-and-signatures\\n\\n    CAVEAT: Some users tend to extract the underlying numpy array and preprocess it for the model using Numpy functions.\\n        The order of the dimensions in the numpy array might not be the same for each back-end or when running a udf locally, \\n        which can lead to unexpected results. \\n\\n        It is recommended to use the named dimensions of the xarray DataArray to avoid this issue.\\n        The order of the dimensions can be changed using the transpose method.\\n        While it is a better practice to do preprocessing using openeo processes, most operations are also available in Xarray. \\n    \\\"\\\"\\\"\\n    # Define how you want to handle nan values\\n    cube = cube.fillna(0)\\n    cube = cube.astype('float32')\\n\\n    # Apply the model for each timestep in the chunk\\n    output_data = apply_model(cube, context)\\n\\n    return output_data\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"dropdimension1\": {\"arguments\": {\"data\": {\"from_node\": \"filterbbox2\"}, \"name\": \"t\"}, \"process_id\": \"drop_dimension\"}, \"filterbands1\": {\"arguments\": {\"bands\": [\"B02_p2\", \"B02_p25\", \"B02_median\", \"B02_p75\", \"B02_p98\", \"B02_mean\", \"B02_sd\", \"B02_sum\", \"B02_iqr\", \"B03_p2\", \"B03_p25\", \"B03_median\", \"B03_p75\", \"B03_p98\", \"B03_mean\", \"B03_sd\", \"B03_sum\", \"B03_iqr\", \"B04_p2\", \"B04_p25\", \"B04_median\", \"B04_p75\", \"B04_p98\", \"B04_mean\", \"B04_sd\", \"B04_sum\", \"B04_iqr\", \"B05_p2\", \"B05_p25\", \"B05_median\", \"B05_p75\", \"B05_p98\", \"B05_mean\", \"B05_sd\", \"B05_sum\", \"B05_iqr\", \"B06_p2\", \"B06_p25\", \"B06_median\", \"B06_p75\", \"B06_p98\", \"B06_mean\", \"B06_sd\", \"B06_sum\", \"B06_iqr\", \"B07_p2\", \"B07_p25\", \"B07_median\", \"B07_p75\", \"B07_p98\", \"B07_mean\", \"B07_sd\", \"B07_sum\", \"B07_iqr\", \"B08_p2\", \"B08_p25\", \"B08_median\", \"B08_p75\", \"B08_p98\", \"B08_mean\", \"B08_sd\", \"B08_sum\", \"B08_iqr\", \"B8A_p2\", \"B8A_p25\", \"B8A_median\", \"B8A_p75\", \"B8A_p98\", \"B8A_mean\", \"B8A_sd\", \"B8A_sum\", \"B8A_iqr\", \"B11_p2\", \"B11_p25\", \"B11_median\", \"B11_p75\", \"B11_p98\", \"B11_mean\", \"B11_sd\", \"B11_sum\", \"B11_iqr\", \"B12_p2\", \"B12_p25\", \"B12_median\", \"B12_p75\", \"B12_p98\", \"B12_mean\", \"B12_sd\", \"B12_sum\", \"B12_iqr\", \"NDVI_p2\", \"NDVI_p25\", \"NDVI_median\", \"NDVI_p75\", \"NDVI_p98\", \"NDVI_mean\", \"NDVI_sd\", \"NDVI_sum\", \"NDVI_iqr\", \"AVI_p2\", \"AVI_p25\", \"AVI_median\", \"AVI_p75\", \"AVI_p98\", \"AVI_mean\", \"AVI_sd\", \"AVI_sum\", \"AVI_iqr\", \"CIRE_p2\", \"CIRE_p25\", \"CIRE_median\", \"CIRE_p75\", \"CIRE_p98\", \"CIRE_mean\", \"CIRE_sd\", \"CIRE_sum\", \"CIRE_iqr\", \"NIRv_p2\", \"NIRv_p25\", \"NIRv_median\", \"NIRv_p75\", \"NIRv_p98\", \"NIRv_mean\", \"NIRv_sd\", \"NIRv_sum\", \"NIRv_iqr\", \"NDMI_p2\", \"NDMI_p25\", \"NDMI_median\", \"NDMI_p75\", \"NDMI_p98\", \"NDMI_mean\", \"NDMI_sd\", \"NDMI_sum\", \"NDMI_iqr\", \"NDWI_p2\", \"NDWI_p25\", \"NDWI_median\", \"NDWI_p75\", \"NDWI_p98\", \"NDWI_mean\", \"NDWI_sd\", \"NDWI_sum\", \"NDWI_iqr\", \"BLFEI_p2\", \"BLFEI_p25\", \"BLFEI_median\", \"BLFEI_p75\", \"BLFEI_p98\", \"BLFEI_mean\", \"BLFEI_sd\", \"BLFEI_sum\", \"BLFEI_iqr\", \"MNDWI_p2\", \"MNDWI_p25\", \"MNDWI_median\", \"MNDWI_p75\", \"MNDWI_p98\", \"MNDWI_mean\", \"MNDWI_sd\", \"MNDWI_sum\", \"MNDWI_iqr\", \"NDVIMNDWI_p2\", \"NDVIMNDWI_p25\", \"NDVIMNDWI_median\", \"NDVIMNDWI_p75\", \"NDVIMNDWI_p98\", \"NDVIMNDWI_mean\", \"NDVIMNDWI_sd\", \"NDVIMNDWI_sum\", \"NDVIMNDWI_iqr\", \"S2WI_p2\", \"S2WI_p25\", \"S2WI_median\", \"S2WI_p75\", \"S2WI_p98\", \"S2WI_mean\", \"S2WI_sd\", \"S2WI_sum\", \"S2WI_iqr\", \"S2REP_p2\", \"S2REP_p25\", \"S2REP_median\", \"S2REP_p75\", \"S2REP_p98\", \"S2REP_mean\", \"IRECI_p2\", \"IRECI_p25\", \"IRECI_median\", \"IRECI_p75\", \"IRECI_p98\", \"IRECI_mean\", \"IRECI_sd\", \"IRECI_sum\", \"IRECI_iqr\", \"VV_p2\", \"VV_p25\", \"VV_median\", \"VV_p75\", \"VV_p98\", \"VV_mean\", \"VV_sd\", \"VV_iqr\", \"VH_p2\", \"VH_p25\", \"VH_median\", \"VH_p75\", \"VH_p98\", \"VH_mean\", \"VH_sd\", \"VH_iqr\", \"RVI_p2\", \"RVI_p25\", \"RVI_median\", \"RVI_p75\", \"RVI_p98\", \"RVI_mean\", \"RVI_sd\", \"RVI_sum\", \"RVI_iqr\", \"VHVVD_p2\", \"VHVVD_p25\", \"VHVVD_median\", \"VHVVD_p75\", \"VHVVD_p98\", \"VHVVD_mean\", \"VHVVD_sd\", \"VHVVD_iqr\", \"VHVVR_p2\", \"VHVVR_p25\", \"VHVVR_median\", \"VHVVR_p75\", \"VHVVR_p98\", \"VHVVR_mean\", \"VHVVR_sd\", \"VHVVR_sum\", \"VHVVR_iqr\"], \"data\": {\"from_node\": \"renamelabels4\"}}, \"process_id\": \"filter_bands\"}, \"filterbands10\": {\"arguments\": {\"bands\": [\"CIRE_p98\", \"CIRE_mean\", \"NDVI_p98\", \"CIRE_iqr\", \"AVI_p98\", \"NIRv_p98\", \"IRECI_iqr\", \"CIRE_median\", \"IRECI_p98\", \"CIRE_sd\", \"CIRE_p75\", \"IRECI_sum\", \"IRECI_sd\", \"NIRv_mean\", \"VH_iqr\", \"IRECI_p75\", \"S2WI_iqr\", \"pop\", \"AVI_p75\", \"NIRv_sd\", \"IRECI_mean\", \"NIRv_p75\", \"NIRv_median\", \"NDVI_mean\", \"IRECI_median\", \"NIRv_iqr\", \"NDVI_p75\", \"BLFEI_p2\", \"NDMI_p75\", \"NDWI_p2\", \"NDMI_p98\", \"NDVI_median\", \"S2REP_p75\", \"NDVIMNDWI_p98\", \"VV_p98\", \"NDWI_p25\", \"NDWI_mean\", \"BLFEI_p25\", \"NDWI_median\", \"BLFEI_mean\", \"VV_p25\", \"NDVIMNDWI_p75\", \"B8A_median\", \"VV_p75\", \"VV_median\", \"AVI_sd\", \"VV_mean\", \"BLFEI_median\", \"BLFEI_p75\", \"NDVI_sd\", \"VH_p75\", \"VH_mean\", \"VH_p25\", \"VH_p98\", \"NDVI_iqr\", \"VH_median\", \"NDWI_sd\", \"B02_p75\", \"S2WI_sum\", \"B04_p2\", \"dist\", \"B11_sum\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands11\": {\"arguments\": {\"bands\": [\"gst\", \"DEM\", \"gdd5\", \"pop\", \"scd\", \"NDVI_median\", \"bdod\", \"B8A_sd\", \"dist\", \"clay\", \"B11_sd\", \"VH_p2\", \"NDVIMNDWI_sd\", \"B06_sd\", \"B07_sd\", \"B05_sum\", \"soc\", \"NDWI_median\", \"NDWI_sd\", \"VHVVR_sd\", \"NDVI_sum\", \"B12_sum\", \"sand\", \"NDWI_sum\", \"NDWI_p25\", \"RVI_sd\", \"bio12\", \"gsp\", \"VV_p2\", \"B04_sd\", \"VHVVR_p75\", \"NDWI_mean\", \"VV_p98\", \"phh2o\", \"B11_sum\", \"AVI_sd\", \"NDVI_p75\", \"VV_p25\", \"cfvo\", \"B05_p75\", \"VV_p75\", \"VH_p25\", \"B12_p75\", \"VV_median\", \"RVI_sum\", \"RVI_mean\", \"NIRv_iqr\", \"VH_mean\", \"MNDWI_sum\", \"B12_sd\", \"VHVVR_p98\", \"NDVIMNDWI_sum\", \"NDVI_p98\", \"VV_mean\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands12\": {\"arguments\": {\"bands\": [\"NDVI_median\", \"NDVI_mean\", \"NDVI_p75\", \"NDVI_p25\", \"NDVIMNDWI_median\", \"CIRE_mean\", \"BLFEI_median\", \"scd\", \"IRECI_median\", \"NDWI_p75\", \"VH_sd\", \"bio12\", \"NDWI_mean\", \"CIRE_p25\", \"NIRv_median\", \"IRECI_p25\", \"dist\", \"NDVIMNDWI_mean\", \"BLFEI_mean\", \"CIRE_median\", \"NDVIMNDWI_p25\", \"NIRv_p25\", \"NDVI_sum\", \"gdd5\", \"gsp\", \"DEM\", \"VH_p2\", \"NDWI_sum\", \"MNDWI_p75\", \"BLFEI_p75\", \"B05_p25\", \"NDVIMNDWI_sum\", \"B06_p25\", \"B8A_p25\", \"B06_median\", \"B07_p25\", \"VV_p2\", \"B08_p25\", \"occur\", \"VV_sd\", \"MNDWI_median\", \"B08_p2\", \"VH_mean\", \"MNDWI_mean\", \"NDVI_p2\", \"NIRv_p2\", \"CIRE_p2\", \"MNDWI_sum\", \"VV_mean\", \"VHVVD_p2\", \"B11_p25\", \"VHVVD_sd\", \"B12_p25\", \"NDWI_p98\", \"B12_p2\", \"S2WI_sum\", \"B11_p2\", \"MNDWI_p98\", \"NDVIMNDWI_p2\", \"S2WI_mean\", \"S2WI_p25\", \"gst\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands13\": {\"arguments\": {\"bands\": [\"B12_p2\", \"NIRv_p2\", \"NDVI_p2\", \"NDWI_p98\", \"S2WI_p25\", \"NDVIMNDWI_p2\", \"B11_p25\", \"B11_p2\", \"CIRE_p2\", \"B11_median\", \"B12_p25\", \"MNDWI_p98\", \"B11_mean\", \"B11_sum\", \"MNDWI_sum\", \"BLFEI_iqr\", \"AVI_p2\", \"BLFEI_p98\", \"B08_p2\", \"B08_p25\", \"B06_sum\", \"B12_sum\", \"B07_sum\", \"BLFEI_sd\", \"B8A_sum\", \"MNDWI_mean\", \"B08_sum\", \"B07_median\", \"BLFEI_sum\", \"IRECI_p2\", \"B05_p2\", \"B06_p2\", \"NDVIMNDWI_sum\", \"B8A_p2\", \"B8A_median\", \"NDWI_sum\", \"B07_p2\", \"B07_mean\", \"B8A_mean\", \"MNDWI_p75\", \"MNDWI_iqr\", \"NIRv_p25\", \"MNDWI_median\", \"IRECI_p25\", \"NIRv_sum\", \"B08_median\", \"B8A_p75\", \"NDVIMNDWI_mean\", \"B08_mean\", \"NIRv_median\", \"NDVIMNDWI_median\", \"NDWI_mean\", \"NDVIMNDWI_p25\", \"NIRv_mean\", \"S2WI_sd\", \"CIRE_sum\", \"NDWI_p75\", \"NDMI_sd\", \"VH_p25\", \"CIRE_p25\", \"NDWI_median\", \"B11_sd\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands14\": {\"arguments\": {\"bands\": [\"gsp\", \"sand\", \"bio12\", \"NIRv_p25\", \"AVI_p25\", \"S2REP_p75\", \"B08_p25\", \"VHVVD_p98\", \"cec\", \"dist\", \"S2REP_median\", \"B03_p25\", \"clay\", \"scd\", \"B12_mean\", \"B05_p75\", \"VV_iqr\", \"B05_p25\", \"B03_median\", \"DEM\", \"B05_median\", \"VHVVR_p2\", \"B11_median\", \"B05_mean\", \"B03_p75\", \"B03_mean\", \"RVI_p2\", \"VHVVD_median\", \"VV_p2\", \"cfvo\", \"VHVVD_mean\", \"B04_mean\", \"S2WI_p98\", \"VHVVD_p25\", \"VH_p2\", \"gdd5\", \"VV_sd\", \"bdod\", \"CIRE_p25\", \"B06_p98\", \"VHVVD_p75\", \"soc\", \"S2WI_sd\", \"AVI_median\", \"VH_sd\", \"NIRv_median\", \"gst\", \"AVI_p2\", \"B07_p25\", \"B06_p25\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands15\": {\"arguments\": {\"bands\": [\"DEM\", \"gdd5\", \"scd\", \"AVI_p25\", \"NIRv_p25\", \"cfvo\", \"gst\", \"B08_p25\", \"soc\", \"AVI_mean\", \"bio12\", \"IRECI_p25\", \"sand\", \"NIRv_median\", \"B02_sd\", \"NDWI_median\", \"B08_p98\", \"IRECI_median\", \"bdod\", \"NDVI_median\", \"CIRE_p25\", \"B03_sd\", \"gsp\", \"phh2o\", \"NDVIMNDWI_median\", \"BLFEI_median\", \"NDVI_p25\", \"clay\", \"NDWI_mean\", \"NDWI_p98\", \"NDWI_p75\", \"B11_p25\", \"BLFEI_sum\", \"B06_mean\", \"AVI_p2\", \"B06_p25\", \"S2REP_median\", \"B8A_p25\", \"B07_p25\", \"VHVVD_mean\", \"NIRv_p2\", \"B11_p98\", \"B08_p2\", \"B06_p75\", \"B08_mean\", \"VH_iqr\", \"AVI_p75\", \"S2WI_median\", \"AVI_median\", \"VHVVD_p25\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands16\": {\"arguments\": {\"bands\": [\"clay\", \"B11_p98\", \"sand\", \"scd\", \"VHVVD_p98\", \"B06_p25\", \"gsp\", \"bio12\", \"RVI_iqr\", \"VV_sd\", \"VH_sd\", \"cec\", \"B11_sum\", \"DEM\", \"dist\", \"gdd5\", \"gst\", \"S2WI_iqr\", \"B11_sd\", \"VV_median\", \"B03_mean\", \"soc\", \"B02_mean\", \"VHVVR_p75\", \"S2WI_median\", \"RVI_p75\", \"NDMI_p75\", \"cfvo\", \"RVI_p98\", \"VHVVR_p98\", \"VHVVR_mean\", \"VHVVR_sum\", \"VV_p75\", \"NDWI_mean\", \"B05_mean\", \"B12_mean\", \"B05_p98\", \"B03_sd\", \"B12_sd\", \"B02_sum\", \"NDVIMNDWI_mean\", \"B02_sd\", \"NIRv_p2\", \"NDVIMNDWI_p2\", \"B11_p75\", \"B02_p25\", \"B04_p98\", \"VHVVR_sd\", \"B02_p98\", \"B12_p98\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands17\": {\"arguments\": {\"bands\": [\"NDWI_p2\", \"B07_p25\", \"bdod\", \"NDVIMNDWI_p98\", \"B06_p25\", \"B8A_p25\", \"B06_median\", \"B08_p25\", \"B02_p2\", \"AVI_p2\", \"MNDWI_p2\", \"S2REP_p98\", \"B06_p2\", \"AVI_p25\", \"MNDWI_iqr\", \"B03_p2\", \"bio12\", \"B8A_median\", \"B07_median\", \"B04_p2\", \"S2REP_mean\", \"B12_p25\", \"B11_p25\", \"B05_p2\", \"VV_p98\", \"S2REP_p75\", \"VH_p98\", \"NIRv_p25\", \"S2REP_median\", \"VH_p75\", \"B12_p2\", \"soc\", \"B08_mean\", \"NDVIMNDWI_sd\", \"B07_mean\", \"NIRv_p2\", \"AVI_mean\", \"B06_mean\", \"BLFEI_p2\", \"NDWI_sd\", \"B8A_mean\", \"BLFEI_iqr\", \"cfvo\", \"NDVI_p98\", \"VH_p2\", \"B03_iqr\", \"B08_p98\", \"B05_iqr\", \"MNDWI_p25\", \"B8A_p98\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands18\": {\"arguments\": {\"bands\": [\"VH_p2\", \"B12_sd\", \"IRECI_p25\", \"gsp\", \"VH_median\", \"scd\", \"DEM\", \"gdd5\", \"BLFEI_p25\", \"NIRv_p25\", \"bio12\", \"phh2o\", \"B04_sum\", \"B07_iqr\", \"BLFEI_median\", \"gst\", \"S2WI_median\", \"B8A_mean\", \"B11_p98\", \"S2WI_p75\", \"RVI_p98\", \"VHVVR_p98\", \"VV_p98\", \"B07_mean\", \"B06_mean\", \"B07_median\", \"VHVVR_sd\", \"B8A_median\", \"VHVVR_p2\", \"dist\", \"VHVVR_iqr\", \"VV_p2\", \"NDMI_p98\", \"B06_median\", \"RVI_iqr\", \"RVI_sd\", \"CIRE_p2\", \"MNDWI_sd\", \"S2WI_mean\", \"NDVIMNDWI_sd\", \"B05_p98\", \"B08_mean\", \"B11_p2\", \"NDWI_p98\", \"S2WI_p25\", \"NIRv_mean\", \"IRECI_sum\", \"NDVIMNDWI_p2\", \"B03_p98\", \"VH_p25\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands19\": {\"arguments\": {\"bands\": [\"bio12\", \"B07_iqr\", \"B12_sd\", \"NIRv_sd\", \"AVI_sd\", \"B05_sd\", \"IRECI_iqr\", \"gsp\", \"NDVI_sum\", \"NDWI_sum\", \"IRECI_sd\", \"NIRv_iqr\", \"B06_iqr\", \"NDVI_iqr\", \"B03_p98\", \"NDVIMNDWI_sum\", \"B02_iqr\", \"IRECI_p98\", \"dist\", \"scd\", \"CIRE_sum\", \"B02_p98\", \"B12_iqr\", \"sand\", \"VHVVD_p25\", \"NDMI_median\", \"VHVVD_p2\", \"NIRv_p98\", \"gdd5\", \"B11_iqr\", \"B03_sd\", \"CIRE_mean\", \"B02_sd\", \"B11_p75\", \"AVI_p98\", \"VH_mean\", \"B11_sd\", \"VH_sd\", \"CIRE_median\", \"NDVI_p75\", \"S2WI_median\", \"NDVI_p98\", \"B12_p75\", \"CIRE_p98\", \"VV_p2\", \"S2WI_iqr\", \"VH_p2\", \"S2WI_sd\", \"VV_mean\", \"B11_median\", \"cec\", \"NDMI_mean\", \"NDMI_p75\", \"VV_sd\", \"B11_mean\", \"B02_p2\", \"B12_p25\", \"NDWI_p2\", \"B05_p25\", \"VHVVD_p75\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands2\": {\"arguments\": {\"bands\": [\"NDVI_p75\", \"BLFEI_p25\", \"NDWI_p25\", \"NDWI_median\", \"NDVIMNDWI_p75\", \"NDVI_median\", \"BLFEI_median\", \"CIRE_p75\", \"B12_sum\", \"bdod\", \"NDVI_mean\", \"NDVIMNDWI_median\", \"CIRE_mean\", \"CIRE_median\", \"phh2o\", \"IRECI_p98\", \"NDWI_mean\", \"IRECI_p75\", \"CIRE_sum\", \"BLFEI_p2\", \"NDVIMNDWI_mean\", \"BLFEI_mean\", \"IRECI_mean\", \"B12_p75\", \"IRECI_sd\", \"NDWI_p75\", \"B04_median\", \"IRECI_median\", \"NDVI_p25\", \"cfvo\", \"NDVI_p98\", \"NDVIMNDWI_p25\", \"CIRE_p98\", \"IRECI_sum\", \"NIRv_p75\", \"BLFEI_p75\", \"NIRv_median\", \"NIRv_p98\", \"NIRv_mean\", \"IRECI_iqr\", \"DEM\", \"B04_p25\", \"NDVIMNDWI_p98\", \"CIRE_p25\", \"gdd5\", \"AVI_p98\", \"B12_mean\", \"AVI_p75\", \"NIRv_sd\", \"soc\", \"VH_p25\", \"VH_iqr\", \"B12_median\", \"B02_median\", \"B04_p75\", \"B12_iqr\", \"NDVI_sum\", \"B11_sum\", \"AVI_mean\", \"B03_median\", \"CIRE_iqr\", \"B04_mean\", \"AVI_median\", \"B02_p75\", \"NIRv_iqr\", \"NIRv_sum\", \"B02_p25\", \"NDWI_p2\", \"BLFEI_sum\", \"B05_p75\", \"VH_mean\", \"B03_p75\", \"CIRE_sd\", \"B04_sum\", \"NDWI_sum\", \"NDMI_sd\", \"NDMI_median\", \"B05_median\", \"VH_median\", \"B05_sum\", \"pop\", \"NDVIMNDWI_sum\", \"NDVI_sd\", \"AVI_p25\", \"NIRv_p25\", \"B8A_p75\", \"S2WI_sum\", \"bio12\", \"NDVI_iqr\", \"VV_p25\", \"MNDWI_median\", \"B12_p25\", \"VH_p75\", \"MNDWI_p25\", \"scd\", \"NDMI_sum\", \"B8A_median\", \"B07_p75\", \"AVI_sd\", \"IRECI_p25\", \"NDMI_p75\", \"B07_iqr\", \"gst\", \"B8A_iqr\", \"VV_mean\", \"B05_mean\", \"B8A_mean\", \"CIRE_p2\", \"B05_p25\", \"B02_mean\", \"S2WI_iqr\", \"B11_p75\", \"B08_p75\", \"NDWI_p98\", \"VV_median\", \"clay\", \"B12_p98\", \"NDMI_mean\", \"NDVI_p2\", \"B8A_p25\", \"B11_mean\", \"B03_mean\", \"AVI_iqr\", \"MNDWI_p75\", \"VH_p2\", \"MNDWI_sum\", \"B06_p75\", \"NDMI_p98\", \"B03_p25\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands20\": {\"arguments\": {\"bands\": [\"gdd5\", \"gst\", \"scd\", \"cfvo\", \"bdod\", \"bio12\", \"B07_sum\", \"dist\", \"MNDWI_sum\", \"B08_sum\", \"BLFEI_sum\", \"AVI_sum\", \"NDVIMNDWI_sum\", \"B06_sum\", \"B8A_sum\", \"DEM\", \"S2WI_sum\", \"S2WI_p2\", \"NDWI_sum\", \"gsp\", \"NIRv_sum\", \"B12_sum\", \"B11_sum\", \"NDVI_sum\", \"soc\", \"B03_sum\", \"NDMI_p2\", \"NDVI_p25\", \"S2WI_p25\", \"B07_p2\", \"NDVIMNDWI_p2\", \"phh2o\", \"AVI_p2\", \"NIRv_p2\", \"NDWI_p98\", \"NDVI_p2\", \"B05_sum\", \"S2WI_mean\", \"B03_p98\", \"B8A_p2\", \"CIRE_p2\", \"S2REP_p75\", \"BLFEI_p98\", \"MNDWI_iqr\", \"MNDWI_sd\", \"B11_iqr\", \"B06_sd\", \"NDVIMNDWI_sd\", \"S2WI_iqr\", \"CIRE_sum\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands21\": {\"arguments\": {\"bands\": [\"gdd5\", \"DEM\", \"scd\", \"bdod\", \"gst\", \"bio12\", \"gsp\", \"soc\", \"cfvo\", \"NDMI_sd\", \"BLFEI_sum\", \"NDVIMNDWI_sum\", \"phh2o\", \"NDVI_iqr\", \"NDWI_sum\", \"AVI_sum\", \"MNDWI_sum\", \"NDMI_iqr\", \"B08_p75\", \"B07_p75\", \"NIRv_sum\", \"B08_sum\", \"B07_sum\", \"B8A_p75\", \"AVI_p75\", \"B06_median\", \"IRECI_sum\", \"NIRv_iqr\", \"B8A_sum\", \"NDVI_sum\", \"NIRv_sd\", \"AVI_median\", \"NIRv_p75\", \"IRECI_sd\", \"IRECI_iqr\", \"NDMI_p2\", \"B08_median\", \"CIRE_iqr\", \"NIRv_median\", \"clay\", \"VV_mean\", \"B06_sum\", \"IRECI_p98\", \"B07_median\", \"NIRv_mean\", \"AVI_iqr\", \"B07_p98\", \"AVI_mean\", \"B06_mean\", \"B08_mean\", \"NDWI_iqr\", \"B8A_iqr\", \"B07_iqr\", \"B8A_mean\", \"B06_p75\", \"CIRE_sd\", \"B07_mean\", \"IRECI_p75\", \"B8A_p98\", \"B03_p75\", \"B8A_median\", \"VV_median\", \"NIRv_p98\", \"BLFEI_iqr\", \"S2WI_p2\", \"B08_p98\", \"cec\", \"VV_p2\", \"AVI_p98\", \"VH_p75\", \"BLFEI_p25\", \"B08_iqr\", \"B02_p75\", \"S2WI_p98\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands22\": {\"arguments\": {\"bands\": [\"gdd5\", \"scd\", \"DEM\", \"bio12\", \"gst\", \"gsp\", \"cfvo\", \"bdod\", \"soc\", \"sand\", \"NDVIMNDWI_sum\", \"BLFEI_sum\", \"NDWI_sum\", \"MNDWI_sum\", \"NDVI_sum\", \"AVI_sum\", \"B8A_sum\", \"B07_sum\", \"cec\", \"B06_sum\", \"NIRv_sum\", \"B11_sum\", \"B08_sum\", \"B12_sum\", \"B05_sum\", \"VV_sd\", \"VHVVD_mean\", \"VH_sd\", \"IRECI_sum\", \"VHVVD_p75\", \"pop\", \"VHVVD_median\", \"VHVVR_sd\", \"VHVVD_p25\", \"VHVVD_p98\", \"RVI_mean\", \"RVI_sum\", \"RVI_median\", \"VHVVR_median\", \"VHVVR_sum\", \"VHVVR_mean\", \"RVI_sd\", \"VH_mean\", \"RVI_p25\", \"dist\", \"VHVVR_p75\", \"B06_iqr\", \"VV_p25\", \"RVI_p75\", \"B04_sum\", \"VHVVR_p25\", \"B03_iqr\", \"B11_p25\", \"S2WI_p25\", \"VH_median\", \"MNDWI_iqr\", \"MNDWI_p75\", \"IRECI_p98\", \"MNDWI_mean\", \"VH_p25\", \"VV_mean\", \"S2WI_mean\", \"S2REP_p75\", \"B03_sum\", \"VV_p98\", \"VH_p98\", \"S2WI_p75\", \"B11_p2\", \"MNDWI_median\", \"B12_p2\", \"S2WI_sum\", \"B02_iqr\", \"B05_iqr\", \"VH_p2\", \"S2WI_median\", \"RVI_p2\", \"B8A_p25\", \"S2REP_p25\", \"clay\", \"CIRE_sd\", \"NDVI_p98\", \"VV_p2\", \"IRECI_p75\", \"NDMI_p98\", \"B07_p25\", \"NDWI_p25\", \"B8A_p75\", \"S2WI_p98\", \"B8A_mean\", \"VHVVR_p2\", \"CIRE_p98\", \"B06_mean\", \"B07_p75\", \"B06_p75\", \"VHVVD_sd\", \"AVI_mean\", \"B08_median\", \"VHVVD_iqr\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands23\": {\"arguments\": {\"bands\": [\"scd\", \"gst\", \"sand\", \"bio12\", \"DEM\", \"gdd5\", \"NDVI_median\", \"NDVIMNDWI_sum\", \"NDWI_sum\", \"NDVI_sum\", \"BLFEI_sum\", \"CIRE_sum\", \"cec\", \"cfvo\", \"CIRE_median\", \"NDWI_median\", \"MNDWI_sum\", \"BLFEI_median\", \"NDVIMNDWI_p25\", \"BLFEI_p75\", \"NDWI_p75\", \"IRECI_sum\", \"CIRE_mean\", \"NDVIMNDWI_median\", \"AVI_sum\", \"NIRv_sum\", \"CIRE_p75\", \"bdod\", \"NDVI_p25\", \"B07_sum\", \"B02_median\", \"B8A_sum\", \"BLFEI_mean\", \"B03_p25\", \"CIRE_p98\", \"soc\", \"NDVI_mean\", \"NDWI_mean\", \"BLFEI_p25\", \"NDVIMNDWI_mean\", \"NDVIMNDWI_p75\", \"NDWI_p25\", \"BLFEI_p2\", \"NDVI_p75\", \"B02_p25\", \"B04_median\", \"MNDWI_p2\", \"B04_p75\", \"B02_p75\", \"B08_sum\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands24\": {\"arguments\": {\"bands\": [\"phh2o\", \"BLFEI_sum\", \"NDVI_sum\", \"MNDWI_sum\", \"NDWI_sum\", \"cfvo\", \"NDVIMNDWI_sum\", \"clay\", \"VH_sd\", \"cec\", \"VV_sd\", \"B05_iqr\", \"B02_p75\", \"B02_iqr\", \"VV_p2\", \"AVI_sum\", \"scd\", \"gdd5\", \"B08_sum\", \"B8A_sum\", \"B05_sd\", \"dist\", \"RVI_p2\", \"VHVVR_p2\", \"B07_sum\", \"VH_p2\", \"NDMI_sum\", \"B06_sum\", \"bio12\", \"gsp\", \"sand\", \"VHVVD_sd\", \"B11_sum\", \"VHVVD_p98\", \"RVI_sd\", \"IRECI_p25\", \"B07_p25\", \"NDVI_p25\", \"B06_p25\", \"NDVI_iqr\", \"CIRE_sum\", \"CIRE_p75\", \"gst\", \"B02_mean\", \"MNDWI_median\", \"B02_sd\", \"B03_mean\", \"BLFEI_sd\", \"VHVVD_iqr\", \"RVI_p25\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands3\": {\"arguments\": {\"bands\": [\"B12_p25\", \"B12_median\", \"B05_sum\", \"B11_p25\", \"B11_median\", \"B12_p75\", \"B05_p75\", \"B04_p25\", \"B05_median\", \"B12_mean\", \"B12_sum\", \"NDMI_median\", \"B12_p2\", \"B04_p2\", \"B04_median\", \"gst\", \"NDMI_mean\", \"B02_p25\", \"B02_p2\", \"NIRv_iqr\", \"B07_iqr\", \"NDMI_sum\", \"DEM\", \"S2WI_p25\", \"B04_sum\", \"B03_p25\", \"B07_p75\", \"IRECI_iqr\", \"scd\", \"B11_p75\", \"B11_sum\", \"B05_mean\", \"NDMI_p25\", \"bio12\", \"B04_p75\", \"S2WI_p98\", \"B08_median\", \"B03_median\", \"B06_p75\", \"NIRv_sd\", \"gsp\", \"B05_p25\", \"B06_iqr\", \"B03_p2\", \"B04_mean\", \"IRECI_sd\", \"B08_p75\", \"S2WI_median\", \"B08_iqr\", \"MNDWI_p75\", \"gdd5\", \"B04_iqr\", \"VH_p75\", \"BLFEI_p75\", \"B12_iqr\", \"S2WI_mean\", \"B11_mean\", \"NDWI_p75\", \"NDMI_p75\", \"B8A_median\", \"B05_p98\", \"B06_median\", \"VV_mean\", \"B02_median\", \"NDMI_p98\", \"B07_median\", \"NDVI_p25\", \"CIRE_p25\", \"B08_p25\", \"B07_mean\", \"B08_mean\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands4\": {\"arguments\": {\"bands\": [\"gdd5\", \"DEM\", \"scd\", \"gst\", \"bio12\", \"cfvo\", \"soc\", \"gsp\", \"sand\", \"VV_p2\", \"NDVI_sum\", \"NDWI_sum\", \"BLFEI_sum\", \"NDVIMNDWI_sum\", \"VH_p2\", \"MNDWI_sum\", \"AVI_sum\", \"VH_sd\", \"CIRE_p25\", \"NDWI_p75\", \"NIRv_sum\", \"NDWI_p98\", \"NDVIMNDWI_p2\", \"CIRE_sum\", \"VV_sd\", \"B02_p98\", \"NDVI_p25\", \"BLFEI_p98\", \"B8A_sum\", \"B03_p98\", \"NDVI_mean\", \"NDVI_median\", \"pop\", \"IRECI_sum\", \"B07_sum\", \"NDWI_mean\", \"B06_sum\", \"phh2o\", \"NIRv_median\", \"clay\", \"NIRv_mean\", \"bdod\", \"NDVI_p2\", \"NIRv_p25\", \"cec\", \"MNDWI_p98\", \"BLFEI_sd\", \"B08_sum\", \"dist\", \"CIRE_median\", \"IRECI_p25\", \"AVI_mean\", \"NIRv_p2\", \"CIRE_mean\", \"IRECI_median\", \"NDWI_median\", \"NDVIMNDWI_p25\", \"S2WI_mean\", \"NDMI_sd\", \"NDVIMNDWI_mean\", \"B11_sum\", \"NDVIMNDWI_iqr\", \"IRECI_p2\", \"NDVI_sd\", \"AVI_p25\", \"NDWI_iqr\", \"NDVIMNDWI_median\", \"S2WI_median\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands5\": {\"arguments\": {\"bands\": [\"scd\", \"bio12\", \"cfvo\", \"soc\", \"DEM\", \"gdd5\", \"bdod\", \"gsp\", \"sand\", \"dist\", \"B08_p25\", \"gst\", \"B8A_p25\", \"B08_median\", \"B06_p25\", \"B07_p25\", \"AVI_p25\", \"B08_mean\", \"AVI_mean\", \"AVI_median\", \"VV_p2\", \"RVI_p2\", \"B07_median\", \"VHVVR_p2\", \"VH_p2\", \"B8A_mean\", \"B06_mean\", \"clay\", \"B06_median\", \"NIRv_p25\", \"B07_mean\", \"NIRv_median\", \"phh2o\", \"B11_sum\", \"B08_sum\", \"AVI_sum\", \"MNDWI_sum\", \"B12_sum\", \"BLFEI_sum\", \"B05_sum\", \"RVI_p25\", \"VHVVR_p25\", \"NDWI_p75\", \"VV_mean\", \"NDVI_p25\", \"RVI_median\", \"VHVVR_median\", \"cec\", \"NDVI_sum\", \"NDVIMNDWI_sum\", \"B11_p75\", \"NDWI_sum\", \"BLFEI_median\", \"RVI_mean\", \"RVI_sum\", \"VH_mean\", \"VHVVR_sum\", \"VHVVR_p75\", \"RVI_p75\", \"B03_p25\", \"B04_p25\", \"NDWI_median\", \"B03_median\", \"VV_p25\", \"VV_p75\", \"VV_median\", \"B11_mean\", \"NDMI_p2\", \"B02_p25\", \"B02_median\", \"B12_median\", \"B11_median\", \"S2WI_p98\", \"B11_p25\", \"VH_median\", \"IRECI_p25\", \"VH_p75\", \"VH_p25\", \"S2WI_sd\", \"VHVVD_p75\", \"B11_iqr\", \"VHVVD_p25\", \"VHVVD_mean\", \"VHVVD_median\", \"pop\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands6\": {\"arguments\": {\"bands\": [\"gdd5\", \"DEM\", \"scd\", \"soc\", \"gst\", \"bio12\", \"cfvo\", \"bdod\", \"B11_sum\", \"MNDWI_sum\", \"phh2o\", \"B12_sum\", \"BLFEI_sum\", \"B8A_sum\", \"AVI_sum\", \"B07_sum\", \"gsp\", \"B08_sum\", \"B06_sum\", \"NDVIMNDWI_sum\", \"S2WI_mean\", \"clay\", \"sand\", \"NDMI_p2\", \"S2WI_p25\", \"NDWI_sum\", \"S2WI_sum\", \"S2WI_p75\", \"S2WI_median\", \"B05_sum\", \"NDMI_p25\", \"NDMI_mean\", \"NIRv_sum\", \"B11_mean\", \"B12_p75\", \"NDVI_sum\", \"B04_sum\", \"B12_mean\", \"cec\", \"NDWI_p75\", \"B11_median\", \"B12_median\", \"NDMI_median\", \"B03_sum\", \"S2WI_p2\", \"B12_p25\", \"B11_p25\", \"NDMI_p98\", \"IRECI_iqr\", \"B11_p75\", \"IRECI_sum\", \"NDVI_mean\", \"NDVI_p25\", \"B02_sum\", \"dist\", \"NDMI_iqr\", \"B12_p2\", \"B02_p75\", \"B02_median\", \"B04_iqr\", \"S2WI_p98\", \"B11_p2\", \"NDWI_mean\", \"B05_p75\", \"CIRE_iqr\", \"NDWI_iqr\", \"B03_p75\", \"pop\", \"MNDWI_mean\", \"IRECI_p25\", \"B04_median\", \"B04_p2\", \"BLFEI_iqr\", \"B04_p25\", \"B05_mean\", \"B03_median\", \"B05_p25\", \"B05_median\", \"MNDWI_p75\", \"NDWI_median\", \"CIRE_sum\", \"IRECI_sd\", \"MNDWI_p2\", \"B05_p2\", \"BLFEI_mean\", \"B02_p25\", \"CIRE_median\", \"B03_p25\", \"B8A_p98\", \"IRECI_p75\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands7\": {\"arguments\": {\"bands\": [\"IRECI_iqr\", \"IRECI_sd\", \"NIRv_p98\", \"IRECI_sum\", \"AVI_p98\", \"CIRE_iqr\", \"NIRv_iqr\", \"B8A_p75\", \"IRECI_p98\", \"B8A_iqr\", \"B07_iqr\", \"IRECI_p75\", \"NIRv_sd\", \"B07_p75\", \"B08_sum\", \"B8A_sum\", \"B07_sum\", \"B11_sum\", \"NDMI_p2\", \"DEM\", \"B08_p98\", \"B8A_median\", \"gdd5\", \"scd\", \"B8A_mean\", \"B07_p98\", \"B11_mean\", \"B8A_p98\", \"AVI_iqr\", \"B08_p75\", \"B11_p75\", \"B11_p25\", \"NIRv_p75\", \"AVI_sum\", \"B07_mean\", \"B08_iqr\", \"B11_median\", \"S2WI_p2\", \"AVI_sd\", \"AVI_p75\", \"B12_sum\", \"IRECI_mean\", \"B06_p75\", \"B06_sum\", \"gst\", \"S2WI_p25\", \"B06_iqr\", \"bio12\", \"B08_sd\", \"MNDWI_sum\", \"B07_median\", \"B07_sd\", \"B8A_sd\", \"S2WI_sum\", \"B12_mean\", \"B06_p98\", \"S2WI_mean\", \"soc\", \"bdod\", \"B06_median\", \"NIRv_mean\", \"B12_p75\", \"B08_mean\", \"B06_sd\", \"S2WI_median\", \"gsp\", \"CIRE_sd\", \"NIRv_median\", \"sand\", \"NDVI_sd\", \"B08_median\", \"BLFEI_p25\", \"MNDWI_median\", \"MNDWI_p75\", \"clay\", \"NDVI_iqr\", \"BLFEI_sum\", \"NDMI_iqr\", \"S2WI_p75\", \"AVI_median\", \"MNDWI_mean\", \"cec\", \"BLFEI_mean\", \"BLFEI_sd\", \"BLFEI_iqr\", \"NDVIMNDWI_sum\", \"cfvo\", \"NDVIMNDWI_p75\", \"NIRv_p2\", \"S2WI_sd\", \"VHVVD_median\", \"B05_median\", \"MNDWI_p25\", \"B04_p75\", \"MNDWI_p2\", \"NDVIMNDWI_sd\", \"CIRE_p75\", \"B04_median\", \"VHVVD_p75\", \"BLFEI_median\", \"NDVI_p98\", \"B03_median\", \"S2REP_p25\", \"NDWI_sum\", \"B02_p25\", \"B05_p2\", \"MNDWI_sd\", \"BLFEI_p75\", \"S2REP_p98\", \"NDWI_median\", \"NDVIMNDWI_p98\", \"NDVI_p75\", \"S2WI_iqr\", \"NDWI_p25\", \"CIRE_sum\", \"VHVVR_p25\", \"B02_median\", \"NDMI_p75\", \"B03_p25\", \"BLFEI_p2\", \"RVI_p25\", \"NDVIMNDWI_median\", \"S2WI_p98\", \"NDVI_median\", \"VHVVD_p98\", \"S2REP_median\", \"MNDWI_iqr\", \"NDVIMNDWI_mean\", \"VHVVD_p25\", \"S2REP_p75\", \"B04_iqr\", \"VV_p2\", \"CIRE_p98\", \"dist\", \"VV_sd\", \"VHVVR_median\", \"NDWI_p75\", \"B02_p75\", \"B07_p2\", \"RVI_median\", \"B03_p2\", \"RVI_mean\", \"RVI_sum\", \"phh2o\", \"NDWI_mean\", \"VV_mean\", \"B11_sd\", \"NDVIMNDWI_p25\", \"B02_mean\", \"B06_p2\", \"VV_p25\", \"VV_median\", \"NDWI_p2\", \"NDVI_sum\", \"RVI_p2\", \"VHVVR_p2\", \"CIRE_median\", \"B06_p25\", \"VH_mean\", \"VHVVR_sum\", \"VHVVR_mean\", \"B03_iqr\", \"VH_sd\", \"VH_p25\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands8\": {\"arguments\": {\"bands\": [\"NDVIMNDWI_sum\", \"BLFEI_sum\", \"NDWI_sum\", \"CIRE_sum\", \"bio12\", \"IRECI_sum\", \"NDVI_sum\", \"AVI_sum\", \"MNDWI_sum\", \"NIRv_sum\", \"B08_sum\", \"scd\", \"gdd5\", \"B11_sum\", \"B8A_sum\", \"DEM\", \"soc\", \"B07_sum\", \"IRECI_mean\", \"NDWI_median\", \"IRECI_p75\", \"CIRE_median\", \"cfvo\", \"B06_sum\", \"NDVI_p75\", \"NIRv_median\", \"NDVI_median\", \"NDWI_mean\", \"gst\", \"CIRE_p75\", \"NDVIMNDWI_mean\", \"NDWI_p25\", \"BLFEI_median\", \"bdod\", \"cec\", \"sand\", \"CIRE_mean\", \"BLFEI_p25\", \"NDVIMNDWI_p75\", \"IRECI_median\", \"NDVI_mean\", \"BLFEI_p2\", \"NDWI_p2\", \"IRECI_p98\", \"IRECI_iqr\", \"NDVIMNDWI_p98\", \"CIRE_iqr\", \"B04_p25\", \"IRECI_sd\", \"BLFEI_mean\", \"MNDWI_median\", \"gsp\", \"B12_sum\", \"MNDWI_p2\", \"NDVI_p98\", \"S2WI_sd\", \"NDVI_p2\", \"S2WI_p2\", \"B03_mean\", \"B02_p2\", \"B03_sum\", \"B02_sum\", \"phh2o\", \"B03_p25\", \"B04_median\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbands9\": {\"arguments\": {\"bands\": [\"bio12\", \"scd\", \"DEM\", \"B12_iqr\", \"gdd5\", \"gsp\", \"B05_iqr\", \"NDMI_p2\", \"NDMI_iqr\", \"soc\", \"cfvo\", \"NDVI_p2\", \"B04_sd\", \"B04_p98\", \"NDVI_p25\", \"BLFEI_p98\", \"NDVIMNDWI_p2\", \"VH_mean\", \"gst\", \"B02_p98\", \"VH_p2\", \"B12_p75\", \"B11_sd\", \"B12_sd\", \"NDVI_sd\", \"NDVI_iqr\", \"NDMI_sd\", \"B04_iqr\", \"NDWI_iqr\", \"S2WI_iqr\", \"NDWI_sd\", \"B04_sum\", \"B12_sum\", \"clay\", \"phh2o\", \"S2WI_p98\", \"S2WI_sd\", \"S2REP_p75\", \"B02_sum\", \"B05_p75\", \"B04_p75\", \"B03_sum\", \"B06_p98\", \"B02_median\", \"sand\", \"B12_mean\", \"B11_p75\", \"B07_p98\", \"NDMI_p25\", \"S2REP_median\"], \"data\": {\"from_node\": \"mergecubes3\"}}, \"process_id\": \"filter_bands\"}, \"filterbbox1\": {\"arguments\": {\"data\": {\"from_node\": \"resamplespatial3\"}, \"extent\": {\"crs\": \"EPSG:3035\", \"east\": 4841500, \"north\": 2818500, \"south\": 2808500, \"west\": 4831500}}, \"process_id\": \"filter_bbox\"}, \"filterbbox2\": {\"arguments\": {\"data\": {\"from_node\": \"resamplespatial4\"}, \"extent\": {\"crs\": \"EPSG:3035\", \"east\": 4841500, \"north\": 2818500, \"south\": 2808500, \"west\": 4831500}}, \"process_id\": \"filter_bbox\"}, \"loadcollection1\": {\"arguments\": {\"bands\": [\"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B11\", \"B12\"], \"id\": \"SENTINEL2_L2A\", \"properties\": {\"eo:cloud_cover\": {\"process_graph\": {\"lte1\": {\"arguments\": {\"x\": {\"from_parameter\": \"value\"}, \"y\": 95}, \"process_id\": \"lte\", \"result\": true}}}}, \"spatial_extent\": {\"crs\": \"EPSG:3035\", \"east\": 4841500, \"north\": 2818500, \"south\": 2808500, \"west\": 4831500}, \"temporal_extent\": [\"2021-01-01\", \"2021-02-01\"]}, \"process_id\": \"load_collection\"}, \"loadcollection2\": {\"arguments\": {\"bands\": [\"SCL\"], \"id\": \"SENTINEL2_L2A\", \"properties\": {\"eo:cloud_cover\": {\"process_graph\": {\"lte2\": {\"arguments\": {\"x\": {\"from_parameter\": \"value\"}, \"y\": 95}, \"process_id\": \"lte\", \"result\": true}}}}, \"spatial_extent\": {\"crs\": \"EPSG:3035\", \"east\": 4841500, \"north\": 2818500, \"south\": 2808500, \"west\": 4831500}, \"temporal_extent\": [\"2021-01-01\", \"2021-02-01\"]}, \"process_id\": \"load_collection\"}, \"loadcollection3\": {\"arguments\": {\"bands\": [\"VH\", \"VV\"], \"id\": \"SENTINEL1_GRD\", \"properties\": {\"sat:orbit_state\": {\"process_graph\": {\"eq1\": {\"arguments\": {\"x\": {\"from_parameter\": \"value\"}, \"y\": \"DESCENDING\"}, \"process_id\": \"eq\", \"result\": true}}}}, \"spatial_extent\": {\"crs\": \"EPSG:3035\", \"east\": 4841500, \"north\": 2818500, \"south\": 2808500, \"west\": 4831500}, \"temporal_extent\": [\"2021-01-01\", \"2021-02-01\"]}, \"process_id\": \"load_collection\"}, \"loadcollection4\": {\"arguments\": {\"bands\": [\"DEM\"], \"id\": \"COPERNICUS_30\", \"spatial_extent\": null, \"temporal_extent\": null}, \"process_id\": \"load_collection\"}, \"loadstac1\": {\"arguments\": {\"url\": \"https://stac.openeo.vito.be/collections/wenr_features\"}, \"process_id\": \"load_stac\"}, \"mask1\": {\"arguments\": {\"data\": {\"from_node\": \"resamplespatial1\"}, \"mask\": {\"from_node\": \"renamelabels1\"}}, \"process_id\": \"mask\"}, \"mergecubes1\": {\"arguments\": {\"cube1\": {\"from_node\": \"renamelabels2\"}, \"cube2\": {\"from_node\": \"renamelabels3\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes10\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes9\"}, \"cube2\": {\"from_node\": \"renamelabels12\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes11\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes10\"}, \"cube2\": {\"from_node\": \"renamelabels13\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes12\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes11\"}, \"cube2\": {\"from_node\": \"renamelabels14\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes13\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes12\"}, \"cube2\": {\"from_node\": \"renamelabels15\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes14\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes13\"}, \"cube2\": {\"from_node\": \"renamelabels16\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes15\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes14\"}, \"cube2\": {\"from_node\": \"renamelabels17\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes16\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes15\"}, \"cube2\": {\"from_node\": \"renamelabels18\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes17\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes16\"}, \"cube2\": {\"from_node\": \"renamelabels19\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes18\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes17\"}, \"cube2\": {\"from_node\": \"renamelabels20\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes19\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes18\"}, \"cube2\": {\"from_node\": \"renamelabels21\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes2\": {\"arguments\": {\"cube1\": {\"from_node\": \"filterbands1\"}, \"cube2\": {\"from_node\": \"filterbbox1\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes20\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes19\"}, \"cube2\": {\"from_node\": \"renamelabels22\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes21\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes20\"}, \"cube2\": {\"from_node\": \"renamelabels23\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes22\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes21\"}, \"cube2\": {\"from_node\": \"renamelabels24\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes23\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes22\"}, \"cube2\": {\"from_node\": \"renamelabels25\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes24\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes23\"}, \"cube2\": {\"from_node\": \"renamelabels26\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes25\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes24\"}, \"cube2\": {\"from_node\": \"renamelabels27\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes3\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes2\"}, \"cube2\": {\"from_node\": \"dropdimension1\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes4\": {\"arguments\": {\"cube1\": {\"from_node\": \"renamelabels5\"}, \"cube2\": {\"from_node\": \"renamelabels6\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes5\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes4\"}, \"cube2\": {\"from_node\": \"renamelabels7\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes6\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes5\"}, \"cube2\": {\"from_node\": \"renamelabels8\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes7\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes6\"}, \"cube2\": {\"from_node\": \"renamelabels9\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes8\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes7\"}, \"cube2\": {\"from_node\": \"renamelabels10\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes9\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes8\"}, \"cube2\": {\"from_node\": \"renamelabels11\"}}, \"process_id\": \"merge_cubes\"}, \"reducedimension1\": {\"arguments\": {\"data\": {\"from_node\": \"loadcollection4\"}, \"dimension\": \"t\", \"reducer\": {\"process_graph\": {\"last1\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"ignore_nodata\": true}, \"process_id\": \"last\", \"result\": true}}}}, \"process_id\": \"reduce_dimension\"}, \"renamelabels1\": {\"arguments\": {\"data\": {\"from_node\": \"toscldilationmask1\"}, \"dimension\": \"bands\", \"target\": [\"S2-L2A-SCL_DILATED_MASK\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels10\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension11\"}, \"dimension\": \"bands\", \"target\": [\"Level2_class-G_164predictors_v1_70100\", \"Level2_class-G_164predictors_v1_70200\", \"Level2_class-G_164predictors_v1_70300\", \"Level2_class-G_164predictors_v1_70400\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels11\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension12\"}, \"dimension\": \"bands\", \"target\": [\"Level2_class-H_65predictors_v1_80200\", \"Level2_class-H_65predictors_v1_80300\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels12\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension13\"}, \"dimension\": \"bands\", \"target\": [\"Level2_class-I_50predictors_v1_90100\", \"Level2_class-I_50predictors_v1_90200\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels13\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension14\"}, \"dimension\": \"bands\", \"target\": [\"Level2_class-J_62predictors_v1_100100\", \"Level2_class-J_62predictors_v1_100200\", \"Level2_class-J_62predictors_v1_100300\", \"Level2_class-J_62predictors_v1_100400\", \"Level2_class-J_62predictors_v1_100600\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels14\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension15\"}, \"dimension\": \"bands\", \"target\": [\"Level2_class-X_54predictors_v1_110400\", \"Level2_class-X_54predictors_v1_110700\", \"Level2_class-X_54predictors_v1_110800\", \"Level2_class-X_54predictors_v1_110900\", \"Level2_class-X_54predictors_v1_112100\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels15\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension16\"}, \"dimension\": \"bands\", \"target\": [\"Level3_class-C1_62predictors_v1_30102\", \"Level3_class-C1_62predictors_v1_30103\", \"Level3_class-C1_62predictors_v1_30104\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels16\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension17\"}, \"dimension\": \"bands\", \"target\": [\"Level3_class-C3_62predictors_v1_30302\", \"Level3_class-C3_62predictors_v1_30304\", \"Level3_class-C3_62predictors_v1_30305\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels17\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension18\"}, \"dimension\": \"bands\", \"target\": [\"Level3_class-E1_50predictors_v1_50101\", \"Level3_class-E1_50predictors_v1_50102\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels18\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension19\"}, \"dimension\": \"bands\", \"target\": [\"Level3_class-E2_50predictors_v1_50201\", \"Level3_class-E2_50predictors_v1_50202\", \"Level3_class-E2_50predictors_v1_50203\", \"Level3_class-E2_50predictors_v1_50206\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels19\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension20\"}, \"dimension\": \"bands\", \"target\": [\"Level3_class-E3_50predictors_v1_50304\", \"Level3_class-E3_50predictors_v1_50305\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels2\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension1\"}, \"dimension\": \"bands\", \"target\": [\"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B11\", \"B12\", \"NDVI\", \"AVI\", \"CIRE\", \"NIRv\", \"NDMI\", \"NDWI\", \"BLFEI\", \"MNDWI\", \"NDVIMNDWI\", \"S2WI\", \"S2REP\", \"IRECI\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels20\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension21\"}, \"dimension\": \"bands\", \"target\": [\"Level3_class-E4_50predictors_v1_50403\", \"Level3_class-E4_50predictors_v1_50404\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels21\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension22\"}, \"dimension\": \"bands\", \"target\": [\"Level3_class-E5_50predictors_v1_50504\", \"Level3_class-E5_50predictors_v1_50505\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels22\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension23\"}, \"dimension\": \"bands\", \"target\": [\"Level3_class-F2_60predictors_v1_60202\", \"Level3_class-F2_60predictors_v1_60203\", \"Level3_class-F2_60predictors_v1_60204\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels23\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension24\"}, \"dimension\": \"bands\", \"target\": [\"Level3_class-F3_50predictors_v1_60301\", \"Level3_class-F3_50predictors_v1_60302\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels24\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension25\"}, \"dimension\": \"bands\", \"target\": [\"Level3_class-G1_74predictors_v1_70101\", \"Level3_class-G1_74predictors_v1_70102\", \"Level3_class-G1_74predictors_v1_70104\", \"Level3_class-G1_74predictors_v1_70105\", \"Level3_class-G1_74predictors_v1_70106\", \"Level3_class-G1_74predictors_v1_70107\", \"Level3_class-G1_74predictors_v1_70108\", \"Level3_class-G1_74predictors_v1_70110\", \"Level3_class-G1_74predictors_v1_70113\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels25\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension26\"}, \"dimension\": \"bands\", \"target\": [\"Level3_class-G3_98predictors_v1_70301\", \"Level3_class-G3_98predictors_v1_70302\", \"Level3_class-G3_98predictors_v1_70304\", \"Level3_class-G3_98predictors_v1_70314\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels26\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension27\"}, \"dimension\": \"bands\", \"target\": [\"Level3_class-H2_50predictors_v1_80203\", \"Level3_class-H2_50predictors_v1_80204\", \"Level3_class-H2_50predictors_v1_80206\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels27\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension28\"}, \"dimension\": \"bands\", \"target\": [\"Level3_class-H3_50predictors_v1_80301\", \"Level3_class-H3_50predictors_v1_80302\", \"Level3_class-H3_50predictors_v1_80306\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels3\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension4\"}, \"dimension\": \"bands\", \"target\": [\"VV\", \"VH\", \"RVI\", \"VHVVD\", \"VHVVR\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels4\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension5\"}, \"dimension\": \"bands\", \"target\": [\"B02_p2\", \"B02_p25\", \"B02_median\", \"B02_p75\", \"B02_p98\", \"B02_mean\", \"B02_sd\", \"B02_sum\", \"B02_iqr\", \"B03_p2\", \"B03_p25\", \"B03_median\", \"B03_p75\", \"B03_p98\", \"B03_mean\", \"B03_sd\", \"B03_sum\", \"B03_iqr\", \"B04_p2\", \"B04_p25\", \"B04_median\", \"B04_p75\", \"B04_p98\", \"B04_mean\", \"B04_sd\", \"B04_sum\", \"B04_iqr\", \"B05_p2\", \"B05_p25\", \"B05_median\", \"B05_p75\", \"B05_p98\", \"B05_mean\", \"B05_sd\", \"B05_sum\", \"B05_iqr\", \"B06_p2\", \"B06_p25\", \"B06_median\", \"B06_p75\", \"B06_p98\", \"B06_mean\", \"B06_sd\", \"B06_sum\", \"B06_iqr\", \"B07_p2\", \"B07_p25\", \"B07_median\", \"B07_p75\", \"B07_p98\", \"B07_mean\", \"B07_sd\", \"B07_sum\", \"B07_iqr\", \"B08_p2\", \"B08_p25\", \"B08_median\", \"B08_p75\", \"B08_p98\", \"B08_mean\", \"B08_sd\", \"B08_sum\", \"B08_iqr\", \"B8A_p2\", \"B8A_p25\", \"B8A_median\", \"B8A_p75\", \"B8A_p98\", \"B8A_mean\", \"B8A_sd\", \"B8A_sum\", \"B8A_iqr\", \"B11_p2\", \"B11_p25\", \"B11_median\", \"B11_p75\", \"B11_p98\", \"B11_mean\", \"B11_sd\", \"B11_sum\", \"B11_iqr\", \"B12_p2\", \"B12_p25\", \"B12_median\", \"B12_p75\", \"B12_p98\", \"B12_mean\", \"B12_sd\", \"B12_sum\", \"B12_iqr\", \"NDVI_p2\", \"NDVI_p25\", \"NDVI_median\", \"NDVI_p75\", \"NDVI_p98\", \"NDVI_mean\", \"NDVI_sd\", \"NDVI_sum\", \"NDVI_iqr\", \"AVI_p2\", \"AVI_p25\", \"AVI_median\", \"AVI_p75\", \"AVI_p98\", \"AVI_mean\", \"AVI_sd\", \"AVI_sum\", \"AVI_iqr\", \"CIRE_p2\", \"CIRE_p25\", \"CIRE_median\", \"CIRE_p75\", \"CIRE_p98\", \"CIRE_mean\", \"CIRE_sd\", \"CIRE_sum\", \"CIRE_iqr\", \"NIRv_p2\", \"NIRv_p25\", \"NIRv_median\", \"NIRv_p75\", \"NIRv_p98\", \"NIRv_mean\", \"NIRv_sd\", \"NIRv_sum\", \"NIRv_iqr\", \"NDMI_p2\", \"NDMI_p25\", \"NDMI_median\", \"NDMI_p75\", \"NDMI_p98\", \"NDMI_mean\", \"NDMI_sd\", \"NDMI_sum\", \"NDMI_iqr\", \"NDWI_p2\", \"NDWI_p25\", \"NDWI_median\", \"NDWI_p75\", \"NDWI_p98\", \"NDWI_mean\", \"NDWI_sd\", \"NDWI_sum\", \"NDWI_iqr\", \"BLFEI_p2\", \"BLFEI_p25\", \"BLFEI_median\", \"BLFEI_p75\", \"BLFEI_p98\", \"BLFEI_mean\", \"BLFEI_sd\", \"BLFEI_sum\", \"BLFEI_iqr\", \"MNDWI_p2\", \"MNDWI_p25\", \"MNDWI_median\", \"MNDWI_p75\", \"MNDWI_p98\", \"MNDWI_mean\", \"MNDWI_sd\", \"MNDWI_sum\", \"MNDWI_iqr\", \"NDVIMNDWI_p2\", \"NDVIMNDWI_p25\", \"NDVIMNDWI_median\", \"NDVIMNDWI_p75\", \"NDVIMNDWI_p98\", \"NDVIMNDWI_mean\", \"NDVIMNDWI_sd\", \"NDVIMNDWI_sum\", \"NDVIMNDWI_iqr\", \"S2WI_p2\", \"S2WI_p25\", \"S2WI_median\", \"S2WI_p75\", \"S2WI_p98\", \"S2WI_mean\", \"S2WI_sd\", \"S2WI_sum\", \"S2WI_iqr\", \"S2REP_p2\", \"S2REP_p25\", \"S2REP_median\", \"S2REP_p75\", \"S2REP_p98\", \"S2REP_mean\", \"S2REP_sd\", \"S2REP_sum\", \"S2REP_iqr\", \"IRECI_p2\", \"IRECI_p25\", \"IRECI_median\", \"IRECI_p75\", \"IRECI_p98\", \"IRECI_mean\", \"IRECI_sd\", \"IRECI_sum\", \"IRECI_iqr\", \"VV_p2\", \"VV_p25\", \"VV_median\", \"VV_p75\", \"VV_p98\", \"VV_mean\", \"VV_sd\", \"VV_sum\", \"VV_iqr\", \"VH_p2\", \"VH_p25\", \"VH_median\", \"VH_p75\", \"VH_p98\", \"VH_mean\", \"VH_sd\", \"VH_sum\", \"VH_iqr\", \"RVI_p2\", \"RVI_p25\", \"RVI_median\", \"RVI_p75\", \"RVI_p98\", \"RVI_mean\", \"RVI_sd\", \"RVI_sum\", \"RVI_iqr\", \"VHVVD_p2\", \"VHVVD_p25\", \"VHVVD_median\", \"VHVVD_p75\", \"VHVVD_p98\", \"VHVVD_mean\", \"VHVVD_sd\", \"VHVVD_sum\", \"VHVVD_iqr\", \"VHVVR_p2\", \"VHVVR_p25\", \"VHVVR_median\", \"VHVVR_p75\", \"VHVVR_p98\", \"VHVVR_mean\", \"VHVVR_sd\", \"VHVVR_sum\", \"VHVVR_iqr\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels5\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension6\"}, \"dimension\": \"bands\", \"target\": [\"Level1_class-0_129predictors_v1_30000\", \"Level1_class-0_129predictors_v1_40000\", \"Level1_class-0_129predictors_v1_50000\", \"Level1_class-0_129predictors_v1_60000\", \"Level1_class-0_129predictors_v1_70000\", \"Level1_class-0_129predictors_v1_80000\", \"Level1_class-0_129predictors_v1_90000\", \"Level1_class-0_129predictors_v1_100000\", \"Level1_class-0_129predictors_v1_110000\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels6\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension7\"}, \"dimension\": \"bands\", \"target\": [\"Level2_class-C_71predictors_v1_30100\", \"Level2_class-C_71predictors_v1_30200\", \"Level2_class-C_71predictors_v1_30300\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels7\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension8\"}, \"dimension\": \"bands\", \"target\": [\"Level2_class-D_68predictors_v1_40100\", \"Level2_class-D_68predictors_v1_40200\", \"Level2_class-D_68predictors_v1_40400\", \"Level2_class-D_68predictors_v1_40500\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels8\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension9\"}, \"dimension\": \"bands\", \"target\": [\"Level2_class-E_85predictors_v1_50100\", \"Level2_class-E_85predictors_v1_50200\", \"Level2_class-E_85predictors_v1_50300\", \"Level2_class-E_85predictors_v1_50400\", \"Level2_class-E_85predictors_v1_50500\", \"Level2_class-E_85predictors_v1_50600\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels9\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension10\"}, \"dimension\": \"bands\", \"target\": [\"Level2_class-F_90predictors_v1_60200\", \"Level2_class-F_90predictors_v1_60300\", \"Level2_class-F_90predictors_v1_60400\", \"Level2_class-F_90predictors_v1_60900\", \"Level2_class-F_90predictors_v1_61100\"]}, \"process_id\": \"rename_labels\"}, \"resamplespatial1\": {\"arguments\": {\"align\": \"upper-left\", \"data\": {\"from_node\": \"loadcollection1\"}, \"method\": \"near\", \"projection\": 3035, \"resolution\": 10}, \"process_id\": \"resample_spatial\"}, \"resamplespatial2\": {\"arguments\": {\"align\": \"upper-left\", \"data\": {\"from_node\": \"sarbackscatter1\"}, \"method\": \"near\", \"projection\": 3035, \"resolution\": 10}, \"process_id\": \"resample_spatial\"}, \"resamplespatial3\": {\"arguments\": {\"align\": \"upper-left\", \"data\": {\"from_node\": \"reducedimension1\"}, \"method\": \"bilinear\", \"projection\": 3035, \"resolution\": 10}, \"process_id\": \"resample_spatial\"}, \"resamplespatial4\": {\"arguments\": {\"align\": \"upper-left\", \"data\": {\"from_node\": \"loadstac1\"}, \"method\": \"near\", \"projection\": 3035, \"resolution\": 10}, \"process_id\": \"resample_spatial\"}, \"sarbackscatter1\": {\"arguments\": {\"coefficient\": \"sigma0-ellipsoid\", \"contributing_area\": false, \"data\": {\"from_node\": \"loadcollection3\"}, \"elevation_model\": \"COPERNICUS_30\", \"ellipsoid_incidence_angle\": false, \"local_incidence_angle\": false, \"mask\": false, \"noise_removal\": true, \"options\": {\"debug\": false, \"elev_geoid\": \"/opt/openeo-vito-aux-data/egm96.tif\", \"implementation_version\": \"2\", \"otb_memory\": 1024, \"tile_size\": 256}}, \"process_id\": \"sar_backscatter\"}, \"saveresult1\": {\"arguments\": {\"data\": {\"from_node\": \"apply4\"}, \"format\": \"GTiff\", \"options\": {\"separate_asset_per_band\": true}}, \"process_id\": \"save_result\", \"result\": true}, \"toscldilationmask1\": {\"arguments\": {\"data\": {\"from_node\": \"loadcollection2\"}, \"erosion_kernel_size\": 3, \"kernel1_size\": 17, \"kernel2_size\": 77, \"mask1_values\": [2, 4, 5, 6, 7], \"mask2_values\": [3, 8, 9, 10, 11], \"scl_band_name\": \"SCL\"}, \"process_id\": \"to_scl_dilation_mask\"}}}, \"progress\": 100, \"status\": \"finished\", \"updated\": \"2024-12-12T14:27:17Z\", \"usage\": {\"cpu\": {\"unit\": \"cpu-seconds\", \"value\": 11888.873007262}, \"duration\": {\"unit\": \"seconds\", \"value\": 1508}, \"input_pixel\": {\"unit\": \"mega-pixel\", \"value\": 57.61798095703125}, \"max_executor_memory\": {\"unit\": \"gb\", \"value\": 2.6082115173339844}, \"memory\": {\"unit\": \"mb-seconds\", \"value\": 58595070.26396463}, \"network_received\": {\"unit\": \"b\", \"value\": 17549675399}}}}</script>\n",
       "    </openeo-job>\n",
       "    "
      ],
      "text/plain": [
       "<BatchJob job_id='j-241212f33620454ba9da2fb2b3b49638'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for i, modelname in enumerate(MODELS_NAMES):\n",
    "    model_url = MODEL_URL + modelname\n",
    "    model_str = Path(modelname).stem + '_'\n",
    "\n",
    "    metadata = get_training_features_from_model(model_url)\n",
    "\n",
    "    input_bands = metadata['input_features']\n",
    "    output_bands = metadata['output_features']\n",
    "    output_bands = [model_str + prob for prob in output_bands]\n",
    "\n",
    "    data_cube_model = data_cube.filter_bands(input_bands)\n",
    "\n",
    "    #we pass the model url as context information within the UDF\n",
    "    udf  = openeo.UDF.from_file(\n",
    "            getUDFpath('udf_catboost_inference.py'),\n",
    "            context={\n",
    "                \"model_url\": model_url\n",
    "                    }\n",
    "    )\n",
    "\n",
    "    # Apply the UDF to the data cube.\n",
    "    catboost_classification = data_cube_model.apply_dimension(\n",
    "        process=udf, dimension = \"bands\")\n",
    "    \n",
    "    #run inference\n",
    "    if i == 0:\n",
    "        output = catboost_classification.rename_labels(dimension=\"bands\",target= output_bands)\n",
    "    else:\n",
    "        bands =  catboost_classification.rename_labels(dimension=\"bands\",target= output_bands)\n",
    "        output = output.merge_cubes(bands)\n",
    "        \n",
    "        \n",
    "output = output.linear_scale_range(0,100, 0,100)\n",
    "\n",
    "#Save each band as a seperate tiff file\n",
    "save_result_options = {}\n",
    "save_result_options[\"separate_asset_per_band\"] = True\n",
    "save_result = output.save_result(\n",
    "                format=\"GTiff\",\n",
    "                options = save_result_options)\n",
    "\n",
    "#create and run the job\n",
    "job = connection.create_job(save_result,\n",
    "    additional=job_options,\n",
    ")\n",
    "\n",
    "job.start_and_wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593d28f",
   "metadata": {},
   "source": [
    "### Step 6: Download the results and plot the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d366f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = job.get_results()\n",
    "results.download_files(\"C:/Git_projects/eo_processing/notebooks/output\")\n",
    "\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to your Cloud Optimized GeoTIFF\n",
    "cog_path = 'C:/Git_projects/eo_processing/notebooks/output/openEO_Level3_class-H2_50predictors_v1_80206.tif'\n",
    "\n",
    "# Open the GeoTIFF\n",
    "with rasterio.open(cog_path) as dataset:\n",
    "    # Read the data\n",
    "    data = dataset.read(1)  # Reading the first band (adjust as necessary)\n",
    "\n",
    "\n",
    "    # Rescale the data between 0 and 1\n",
    "\n",
    "    # Print min and max values of the rescaled data\n",
    "    print(f\"Rescaled Minimum Value: {np.min(data)}\")\n",
    "    print(f\"Rescaled Maximum Value: {np.max(data)}\")\n",
    "\n",
    "    # Plot the rescaled data\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(data, cmap='viridis')\n",
    "    plt.colorbar(label='Normalized Value')\n",
    "    plt.title(\"GeoTIFF Visualization (Rescaled)\")\n",
    "    plt.xlabel(\"Column Index\")\n",
    "    plt.ylabel(\"Row Index\")\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weed_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
